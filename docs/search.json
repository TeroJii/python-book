[
  {
    "objectID": "control_flow.html",
    "href": "control_flow.html",
    "title": "2  Control Flow Statements",
    "section": "",
    "text": "2.1 The if statement\nThe if statement evaluates a condition and based on it’s value (True or False) it will execute or skip a block of code.\nif True:\n    print('This will be printed')\n\nif False:\n    print('This will not be printed')\n\nThis will be printed\nWe are not limited to using boolean values in the condition. We can use any expression that evaluates to a boolean value.\nname = 'Tero'\n\nif name == 'Tero':\n    # code block to be executed if the condition is True\n    print('Hello Tero!')\n\nHello Tero!\nYou should note that unlike some other programming languages, Python does not use curly braces to define code blocks. Instead, Python uses indentation. The code block should be indented with 4 spaces. Most (all?) modern code editors should handle this automatically, so you needn’t worry about it too much.",
    "crumbs": [
      "The Very Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Control Flow Statements</span>"
    ]
  },
  {
    "objectID": "control_flow.html#the-if-statement",
    "href": "control_flow.html#the-if-statement",
    "title": "2  Control Flow Statements",
    "section": "",
    "text": "2.1.1 Extending the if statement with elif and else\nWhat if we want to execute some code also if the condition is False? We can use the else statement for that.\n\nname = 'Antero'\n\nif name == 'Tero':\n    print('Hello Tero!')\nelse:\n    print('Hello stranger!')\n\nHello stranger!\n\n\nAnd in the case of multiple conditions, we can use the elif statement to check for additional conditions if the previous conditions were False.\n\nname = 'Antero'\n\nif name == 'Tero':\n    print('Hello Tero!')\nelif name == 'Antero':\n    print(\"Oh, it's you again!\")\nelse:\n    print('Hello stranger!')\n\nOh, it's you again!\n\n\nThat is the basic idea behind the if-else statement. You can have as many elif statements as you want, but only one (or none) else statement which is located at the end. Conditions can also be nested.\n\nname = 'Tero'\nage = 30\n\nif name == 'Tero':\n    if age &lt; 18:\n        print('Hello young Tero!')\n    else:\n        print('Hello adult Tero!')\nelse:\n    print('Hello stranger!')\n\nHello adult Tero!",
    "crumbs": [
      "The Very Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Control Flow Statements</span>"
    ]
  },
  {
    "objectID": "control_flow.html#the-for-loop",
    "href": "control_flow.html#the-for-loop",
    "title": "2  Control Flow Statements",
    "section": "2.2 The for loop",
    "text": "2.2 The for loop\nThe idea behind a loop structure is to repeat a block of code multiple times. The for loop is used when we know how many times we want to repeat the code block.\n\nseasons = ['Spring', 'Summer', 'Autumn', 'Winter']\n\nfor season in seasons:\n    print('It is now {}'.format(season))\n\nIt is now Spring\nIt is now Summer\nIt is now Autumn\nIt is now Winter\n\n\nThe for loop iterates over the elements of the seasons list. In each iteration, the variable season is assigned the value of the current element. The loop continues until all elements have been iterated over. This is useful for example when we need to repeat a calculation several times.\n\nsquared_numbers = []\n\nfor i in range(1, 6):\n    squared_numbers.append(i**2)\n\nprint(squared_numbers)\n\n[1, 4, 9, 16, 25]",
    "crumbs": [
      "The Very Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Control Flow Statements</span>"
    ]
  },
  {
    "objectID": "control_flow.html#the-while-loop",
    "href": "control_flow.html#the-while-loop",
    "title": "2  Control Flow Statements",
    "section": "2.3 The while loop",
    "text": "2.3 The while loop\nThe while loop is the for loop’s liberal cousin. It is less restrictive and repeats a block of code as long as a condition is True. Depending on the condition we might not know how many times the loop will be executed beforehand. The price for this freedom is that we might accidentally create an infinite loop if we’re not careful. Let’s look at a simple example.\n\ncount = 0\n\nwhile count &lt; 5:\n    print('Count is {}'.format(count))\n    count += 1\n\nCount is 0\nCount is 1\nCount is 2\nCount is 3\nCount is 4\n\n\nThe example above could have been implemented with a for loop as well. As a rule of thumb a forloop can always be written as a while loop, but not the other way around. We’ll come back to more complicated while loops later, when we learn about random number generation and the break and continue statements.",
    "crumbs": [
      "The Very Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Control Flow Statements</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Basic Data Types\nNumbers, strings and booleans are some of the most basic data types found in Python. Let’s quickly go through some basic operations that can be performed on these data types.",
    "crumbs": [
      "The Very Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#basic-data-types",
    "href": "intro.html#basic-data-types",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1.1 Working with numbers\nNumbers can be categorized into integers and floats, depending on the use of a decimal point. Both can be used for performing basic arithmetics as shown below:\n\n1 + 1\n\n2\n\n\n\n1.0 + 2.5\n\n3.5\n\n\nThis as such is not super useful. However, we can assign numbers (as well as other data types) into variables, which will help us store and manipulate data:\n\nmy_iq = 259\n\nprint(my_iq)\n\n259\n\n\nWhen we input large numbers, we can use underscores to make them more readable:\n\nnumber_of_people_reading_this_book = 1_000_000\n\nnumber_of_people_reading_this_book\n\n1000000\n\n\nBasic arithmetic operations can be performed on numbers using the following syntax:\n\na = 10\nb = 3\n\n# addition\na + b\n\n13\n\n\n\n# subtraction\na - b\n\n7\n\n\n\n# multiplication\na * b\n\n30\n\n\n\n# division\na / b\n\n3.3333333333333335\n\n\n\n# floor division\na // b\n\n3\n\n\n\n# modulo\na % b\n\n1\n\n\n\n# exponentiation\na ** b\n\n1000\n\n\n\n\n\n\n\n\nCaution\n\n\n\nWhen working with floating point numbers, we might encounter some precision issues stemming from the way computers store numbers. For example, you would expect the following code to return 0.0, but it doesn’t due to the aforementioned limitations:\n\n1 - 0.7 - 0.3\n\n5.551115123125783e-17\n\n\nThis is usually not a problem, but it’s good to keep in mind nevertheless.\n\n\n\n\n1.1.2 Strings\nStrings are used to represent text data. They can be enclosed in either single ' or double \" quotes. Some basic operations on strings include concatenation and repetition:\n\n\"Hello\" + \" \" + \"World\"\n\n'Hello World'\n\n\n\n\"Hello\" * 3\n\n'HelloHelloHello'\n\n\nStrings can also be indexed and sliced:\n\nmy_string = \"Hello World\"\n\n# indexing\nprint(my_string[0])\n\nH\n\n\n\n# slicing\nprint(my_string[0:5])\n\nHello\n\n\nSometimes we might have to convert numbers to strings and vice versa. This can be done using the str() and int() functions:\n\n# converting a number to a string\nstr(123)\n\n'123'\n\n\n\nstring_disguised_as_number = '123'\n\n# converting a string to a number\nint(string_disguised_as_number)\n\n123\n\n\nIn case of floats, we can use the float() function:\n\n# converting a string to a float\nfloat('3.14')\n\n3.14\n\n\nIf you need to create a string which spans multiple lines, you can use triple quotes to do so:\n\nmultiline_string =\"\"\"\nThis is a string\nthat spans multiple\nlines\n\"\"\"\nprint(multiline_string)\n\n\nThis is a string\nthat spans multiple\nlines\n\n\n\nThe three quotation marks are basically just a shorthand for creating a string with newline characters in it, as we can see if we print out the string variable:\n\nmultiline_string\n\n'\\nThis is a string\\nthat spans multiple\\nlines\\n'\n\n\nSometimes it is useful to format strings in a certain way. This can be done using the format() method:\n\nname = \"John\"\nage = 25\nformatted_string = \"My name is {} and I am {} years old\".format(name, age)\n\nprint(formatted_string)\n\nMy name is John and I am 25 years old\n\n\nThere are other intricacies related to working with strings, such as the split() and join() methods, which we will cover in a later section… maybe.\n\n\n1.1.3 Booleans\nBooleans are used to represent truth values, namely True and False. They can be used in conjunction with logical operators such as and, or and not:\n\nTrue and False\n\nFalse\n\n\nBooleans are most commonly used in conditional statements, which we will cover in a later section.",
    "crumbs": [
      "The Very Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#other-data-types",
    "href": "intro.html#other-data-types",
    "title": "1  Introduction",
    "section": "1.2 Other data types",
    "text": "1.2 Other data types\nPython has a number of built-in data structures that can be used to store collections of data. Some of the most commonly used ones are lists, tuples, sets and dictionaries.\n\n1.2.1 Lists\nLists are used to store collections of items. They are ordered, mutable and can contain items of different types. Lists are defined using square brackets []:\n\nmy_list = [1, 'two', True, False, 5]\n\nprint(my_list)\n\n[1, 'two', True, False, 5]\n\n\nYou can access elements in a list using their index. Just remember that Python uses zero-based indexing:\n\n# get second element\nmy_list[1]\n\n'two'\n\n\nYou can also slice lists, i.e. get a subset of the list using the following syntax:\n\n# get first three elements\nmy_list[:3]\n\n[1, 'two', True]\n\n\nNegative indices can be used to access elements from the end of the list:\n\n# get the last element\nmy_list[-1]\n\n5\n\n\nA list can also contain nested lists:\n\nnested_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\nprint(nested_list)\n\n[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n\nAccessing the elements of a nested list is done by chaining the index operators:\n\nnested_list[1][2]\n\n6\n\n\nThis can be tricky at times. Consider yourself warned.\nAdding new elements to a list can be done using the append() method:\n\nmy_list.append('New element')\n\nprint(my_list)\n\n[1, 'two', True, False, 5, 'New element']\n\n\nWe can return the last element from a list using the pop() method:\n\nmy_list.pop()\n\n'New element'\n\n\nThis will also remove it from the list as we can see by inspecting the list again:\n\nprint(my_list)\n\n[1, 'two', True, False, 5]\n\n\nIf we want to remove a specific element from our list, we can do so by using the remove() method:\n\nmy_list.remove('two')\n\nprint(my_list)\n\n[1, True, False, 5]\n\n\n\n\n1.2.2 Tuples\nTuples are kind of like lists, but they are immutable, which is a fancy way of saying that once they are created, their size and contents cannot be changed. Tuples are defined using parentheses ():\n\n# creating a tuple\nmy_tuple = (1, 'two', True, False, 5)\n\nmy_tuple\n\n(1, 'two', True, False, 5)\n\n\nYou can access elements in a tuple using their index, just like with lists:\n\n# get second element\nmy_tuple[1]\n\n'two'\n\n\nWhy would you then create a tuple instead of a list? Well, tuples are faster than lists, and I guess sometimes you want to make sure that the data you are working with doesn’t change to name a few reasons.\n\n\n1.2.3 Sets\nSets are kind of like lists or tuples, but they are unordered and do not allow duplicate elements. Sets are defined using curly braces {}:\n\nmy_set = {1, 2, 3}\n\nmy_set\n\n{1, 2, 3}\n\n\nThere is also a set() function can be used to create a set, but we won’t go into that here. Sets are mutable, so you can add and remove elements from them. They are also useful for performing set operations such as union, intersection, difference and symmetric difference:\n\nset1 = {1, 2, 3}\nset2 = {3, 4, 5}\n\n# union\nset1 | set2\n\n{1, 2, 3, 4, 5}\n\n\n\n# intersection\nset1 & set2\n\n{3}\n\n\n\n# difference\n\nset1 - set2\n\n{1, 2}\n\n\n\n# symmetric difference\nset1 ^ set2\n\n{1, 2, 4, 5}\n\n\n\n\n1.2.4 Dictionaries\nLast but not least we have dictionaries, which are used to store key-value pairs. Dictionaries are unordered, mutable and can contain items of different types. Dictionaries are defined using curly braces {}:\n\nmy_dict = {'name': 'Tero', 'likes': 'Pizza', 'is_student': False, 'age': 25}\n\nYou can access the value of a key in a dictionary using the key itself:\n\nmy_dict['name']\n\n'Tero'\n\n\nThat’s it. We have covered the basics of Python data types. Phew! Pat yourself on the back for making it this far, and treat yourself to a cup of coffee or a slice of pizza. You deserve it! Next up, we will cover control flow statements in Python. Exciting… I know!",
    "crumbs": [
      "The Very Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "functions.html",
    "href": "functions.html",
    "title": "3  Functions",
    "section": "",
    "text": "3.1 Built-in functions\nLet’s take a look at a few built-in functions and how they can be used. Here are a few examples (some of which we’ve already encountered):\nUsing basic functions is dead simple. Usually the function has some arguments which the user passes on to the function as input. Let’s try the functions above to get a feel for how they work:\nmy_list = [\"Hello\", \"I'm\", 1, \"list\", True]\n\nprint(my_list)\n\n['Hello', \"I'm\", 1, 'list', True]\nSo, we see that the print() function can be used to print out the contents of the input parameter into the console. Makes sense, right?\nIf we inspect the type of the my_list variable, we can (rather unsurprisingly) see that it is a list:\ntype(my_list)\n\nlist\nWe can also confirm that the third element of the list is an integer:\ntype(my_list[2])\n\nint\nAnd finally, our list contains 5 elements in total, as we can see when we pass my_list to the len() function:\nlen(my_list)\n\n5\nThat’s the basics of using built-in functions. There are loads of other functions, but we won’t cover them here. They are best learned when needed. Next we’ll see how to build our own functions.",
    "crumbs": [
      "The Very Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#built-in-functions",
    "href": "functions.html#built-in-functions",
    "title": "3  Functions",
    "section": "",
    "text": "print(): prints the given argument(s) to the console\ntype(): returns the type of the given argument\nlen(): returns the length of the given argument",
    "crumbs": [
      "The Very Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#user-defined-functions",
    "href": "functions.html#user-defined-functions",
    "title": "3  Functions",
    "section": "3.2 User-defined functions",
    "text": "3.2 User-defined functions\nCustom-made functions are a way to encapsulate code that you want to reuse. They are defined using the def keyword, followed by the function name, and a colon.\n\n# creating a custom function\ndef my_function():\n    # enter the code you wish to run below\n    # note that the indentation is important here as well\n    print(\"Hello I'm a custom-made function!\")\n\n# calling the function\nmy_function()\n\nHello I'm a custom-made function!\n\n\nAlthough the function above isn’t particulrly useful, it is still a valid function. We can make our functions more useful by adding some parameters when defining them.\n\n# creating a custom function with parameters\ndef my_function_with_args(name):\n    print(f\"Hello {name}, I'm a custom-made function with arguments!\")\n\n# we can now pass an argument to the function when calling it\nmy_function_with_args(\"John\")\n\nHello John, I'm a custom-made function with arguments!\n\n\nWe can also return values from functions. This is done using the return keyword.\n\ndef num_squared(num = 2):\n    returned_value = num ** 2\n    return returned_value\n\nnum_squared()\n\n4\n\n\nAs we saw above, we can also set default values for the parameters of the function. The default value will be used if the user doesn’t pass any arguments to the function.\n\n\n\n\n\n\nNote\n\n\n\nVariables defined inside a function are not accessible outside of it. For example, we can’t directly call the returned_value variable outside of the num_squared function. This is called the scope of the variable.\n\n\n\n3.2.1 Documenting your function\nIt is a good practice to document your functions. This is done by adding something called a docstring to the function. A docstring is a string that is placed at the beginning of the function and is enclosed in triple quotes. You can include text that describes what the function does and how it is used, i.e., what arguments it takes, and what it returns. This is especially useful with more complex functions.\n\ndef my_function_with_args(name):\n    \"\"\"\n    This function takes a name as an argument and prints a greeting.\n    \"\"\"\n    print(f\"Hello {name}, I'm a custom-made function with arguments!\")\n\nThe docstring can be accessed by using the __doc__ attribute of the function.\n\nprint(my_function_with_args.__doc__)\n\n\n    This function takes a name as an argument and prints a greeting.",
    "crumbs": [
      "The Very Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#lambda-expressions",
    "href": "functions.html#lambda-expressions",
    "title": "3  Functions",
    "section": "3.3 Lambda expressions",
    "text": "3.3 Lambda expressions\nLamdas are a way to create small, anonymous functions. They are defined using the lambda keyword, followed by the arguments, a colon, and the expression to evaluate.\nLet’s say we have a simple function, which can be described in one line of code. We can naturally define a proper function like this:\n\ndef add(a, b):\n    return a + b\n\nHowever, we can also accomplish the same by using a lambda expression:\n\nlambda a, b: a + b\n\n&lt;function __main__.&lt;lambda&gt;(a, b)&gt;\n\n\nWe can use the following syntax to pass arguments to the lambda expression for evaluation:\n\n(lambda a, b: a + b)(2, 3)\n\n5\n\n\nNow, why would we want to do this you might ask?\nLet’s say we want to raise a bunch of numbers to the second power. We could do this by first defining a function and the passing it on to the map() function, which applies the function to each element of a list.\n\ndef squared(x):\n    return x ** 2\n\nlist(map(squared, [1, 2, 3, 4, 5]))\n\n[1, 4, 9, 16, 25]\n\n\nThis works, and is perfectly fine. However, the squared() function is quite simple, so it seems a bit silly to write a separate function for this. Here is where the lambda expression comes in handy. It allows us to accomplish the same thing in a more concise way:\n\n# same with a lambda expression\nlist(map(lambda x: x ** 2, [1, 2, 3, 4, 5]))\n\n[1, 4, 9, 16, 25]\n\n\nWe get the same result, but with less code. Lambda expressions are especially useful when you need to pass a simple function as an argument to another function. We will work more with them once we familiarize ourselves with Pandas DataFrames.",
    "crumbs": [
      "The Very Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "functions.html#methods",
    "href": "functions.html#methods",
    "title": "3  Functions",
    "section": "3.4 Methods",
    "text": "3.4 Methods\nWhat are methods and how do they differ from functions? In Python, methods are functions that are associated with an object. They are called using the dot notation, i.e., object.method(). A common methods associated with strings is the upper() method, which converts all characters in a string to uppercase. Let’s take a look shall we?\n\nmy_string = \"hello, I'm a string\"\n\nmy_string.upper()\n\n\"HELLO, I'M A STRING\"\n\n\nYou can browse methods associated with an object by using the dir() function. This will return a list of all the methods associated with the object.\n\n# storing the methods associated with the my_string object in a variable\nmethods_for_my_string = dir(my_string)\n\n# printing the last 5 methods\nmethods_for_my_string[-5:]\n\n['swapcase', 'title', 'translate', 'upper', 'zfill']\n\n\nYou can also use the help() function to get more information about a method. This will return a description of the method, as well as the arguments it takes.\n\nhelp(my_string.upper)\n\nHelp on built-in function upper:\n\nupper() method of builtins.str instance\n    Return a copy of the string converted to uppercase.\n\n\n\nWhat makes methods so powerful is that they can be chained together. This means that you can call multiple methods on the same object in a single line of code. Let’s see an example of this:\n\nmy_string.upper().split()\n\n['HELLO,', \"I'M\", 'A', 'STRING']",
    "crumbs": [
      "The Very Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "pandas.html",
    "href": "pandas.html",
    "title": "5  Pandas Basics",
    "section": "",
    "text": "5.1 Series\nSeries is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index. The basic method to create a Series is to call the series constructor:\n# import numpy and pandas libraries\nimport numpy as np\nimport pandas as pd\n\n# create a series from a list\npd.Series(data = [1, 2, 3, 4])\n\n0    1\n1    2\n2    3\n3    4\ndtype: int64\nWe can see that the series look very much like the list or a NumPy array. The series also has an index that can be used to access the elements of the series, but the difference is that we can specify the index values for our series:\n# create a series with custom index\nmy_series = pd.Series(data = [1, 2, 3, 4], index = ['a', 'b', 'c', 'd'])\nmy_series\n\na    1\nb    2\nc    3\nd    4\ndtype: int64\nWe can access the elements of the series using the index values:\n# access the elements of the series\nmy_series['a']\n\n1\nYou can also use the dot notation:\n# access the elements of the series using the dot notation\nmy_series.b\n\n2",
    "crumbs": [
      "Basic Data Manipulation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Pandas Basics</span>"
    ]
  },
  {
    "objectID": "linear_regression.html",
    "href": "linear_regression.html",
    "title": "9  Linear regression",
    "section": "",
    "text": "9.1 Simple linear regression\nSimple linear regression is the most basic form of regression. It is used when we want to use the values of a single independent variable to predict the values for the dependent variable. To put it simply, the goal is to find the best-fitting straight line through the data. The equation for a simple linear regression model is:\n\\[\ny = \\beta_0 + \\beta_1x \\text{,}\n\\tag{9.1}\\]\nwhere \\(y\\) is the dependent variable, \\(x\\) is the independent variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the slope.\nLet’s see how we can implement simple linear regression in Python using the scikit-learn library. Scikit-learn is one of the most fundamental machine learning libraries for Python. It provides a wide range of tools for building machine learning models, including regression models. Before we start, we need to install the library. You can do this by running the following command in your terminal:\nAfter installation, we are ready to implement a simple linear regression model. We will use data from the World Happiness Report 2019, which is available through Kaggle (Kaggle Datasets (2024)). The World Happiness Report is a survey of the state of global happiness that ranks 156 countries by how happy their citizens perceive themselves to be. The dataset contains information about various factors that contribute to happiness, such as GDP per capita, social support, healthy life expectancy, freedom to make life choices, generosity, and perceptions of corruption (Table 9.1).\nimport pandas as pd\nimport seaborn as sns\n\n# get the diamonds dataset\nhappiness = pd.read_csv('data/WorldHappinessReport2019.csv')\nhappiness.head()\n\n\n\nTable 9.1: The first rows of the World Happiness Report 2019 dataset.\n\n\n\n\n\n\n\n\n\n\n\nOverall rank\nCountry or region\nScore\nGDP per capita\nSocial support\nHealthy life expectancy\nFreedom to make life choices\nGenerosity\nPerceptions of corruption\n\n\n\n\n0\n1\nFinland\n7.769\n1.340\n1.587\n0.986\n0.596\n0.153\n0.393\n\n\n1\n2\nDenmark\n7.600\n1.383\n1.573\n0.996\n0.592\n0.252\n0.410\n\n\n2\n3\nNorway\n7.554\n1.488\n1.582\n1.028\n0.603\n0.271\n0.341\n\n\n3\n4\nIceland\n7.494\n1.380\n1.624\n1.026\n0.591\n0.354\n0.118\n\n\n4\n5\nNetherlands\n7.488\n1.396\n1.522\n0.999\n0.557\n0.322\n0.298\nFigure 9.1 visualizes the relationship between a country’s happiness Score and GDP per capita. We can see that, on average countries with higher GDP per capita exhibit higher levels of happiness.\nsns.lmplot(x='GDP per capita', y='Score', data=happiness)\n\n\n\n\n\n\n\nFigure 9.1: A scatter plot showing the relationship between the happiness score and the GDP per capita. Note that correlation does not imply causation.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "11  Summary",
    "section": "",
    "text": "11.1 What’s next?",
    "crumbs": [
      "Wrapping up",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python Essentials for Wrestling with Data",
    "section": "",
    "text": "Preface\n\n\nCode\nprint('Hello World!')\n\n\nHello World!\n\n\nSometimes wrestling with data can really bite. However, it doesn’t need to be that way. Python is a powerful tool for data analysis and visualisation, and it can help extract insight from your data and communicate them to the world. This book is meant to give you an overview of the core concepts in Python that are useful for data analysis. The approach aims to be pragmatic and hands-on, with concrete examples to help you get started. This should give you a solid foundation to build upon as you advance on your Python journey. It will help if you have some basic programming experience. Luckily, there are plenty of resources available online to help you if you get stuck. Just remember that programming is a skill that you need to keep practicing to get better at it.\nSo, who am I? My name is Tero and I work as a data science consultant. Previously, I have worked as a researcher in the pharmaceutical industry and academia. Over the years I have used several tools for analysing data. With the rise of data science and deep learning in particular, Python has become the most used programming language in the world. These days, you don’t really need to look for reasons to learn Python. I started to write this book for myself, to have a reference to some core concepts in Python. If someone else finds it useful, that’s great!\n\n\n\n\n\n\nNote\n\n\n\nPlease note that this book is currently work in progress. That being said, some chapters should already serve as decent references and/or introductions to certain topics. My hope and aim is that this book will serve as a useful reference to many core concepts in using Python for analyzing data.\nShould you find the current state of this book too unpolished, take comfort in the fact that many excellent books on Python programming have been written. For example, Wes McKinney’s Python for data analysis will give an in-depth view on many Python concepts (McKinney (2022)).\n\n\n\n\n\n\n\n\nAbout the Online Version\n\n\n\nYou are currently reading the free online version of this book. You can find the source code for this book on GitHub. If you find this book useful, please consider supporting the author by purchasing a copy on TO BE ANNOUNCED. Thank you!\n\n\n\n\n\n\nMcKinney, Wes. 2022. Python for Data Analysis. O’Reilly Media. https://wesmckinney.com/book/.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "functions.html#using-help-for-functions-and-classes",
    "href": "functions.html#using-help-for-functions-and-classes",
    "title": "3  Functions",
    "section": "3.5 Using help() for functions and classes",
    "text": "3.5 Using help() for functions and classes\nHelp is a great way to get more information about a method, especially when you are not sure how to use it. It also works for functions and classes.\n\nhelp(help)\n\nHelp on _Helper in module _sitebuiltins object:\n\nclass _Helper(builtins.object)\n |  Define the builtin 'help'.\n |  \n |  This is a wrapper around pydoc.help that provides a helpful message\n |  when 'help' is typed at the Python interactive prompt.\n |  \n |  Calling help() at the Python prompt starts an interactive help session.\n |  Calling help(thing) prints help for the python object 'thing'.\n |  \n |  Methods defined here:\n |  \n |  __call__(self, *args, **kwds)\n |      Call self as a function.\n |  \n |  __repr__(self)\n |      Return repr(self).\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables\n |  \n |  __weakref__\n |      list of weak references to the object\n\n\n\n\nhelp(list)\n\nHelp on class list in module builtins:\n\nclass list(object)\n |  list(iterable=(), /)\n |  \n |  Built-in mutable sequence.\n |  \n |  If no argument is given, the constructor creates a new empty list.\n |  The argument must be an iterable if specified.\n |  \n |  Methods defined here:\n |  \n |  __add__(self, value, /)\n |      Return self+value.\n |  \n |  __contains__(self, key, /)\n |      Return key in self.\n |  \n |  __delitem__(self, key, /)\n |      Delete self[key].\n |  \n |  __eq__(self, value, /)\n |      Return self==value.\n |  \n |  __ge__(self, value, /)\n |      Return self&gt;=value.\n |  \n |  __getattribute__(self, name, /)\n |      Return getattr(self, name).\n |  \n |  __getitem__(...)\n |      x.__getitem__(y) &lt;==&gt; x[y]\n |  \n |  __gt__(self, value, /)\n |      Return self&gt;value.\n |  \n |  __iadd__(self, value, /)\n |      Implement self+=value.\n |  \n |  __imul__(self, value, /)\n |      Implement self*=value.\n |  \n |  __init__(self, /, *args, **kwargs)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __iter__(self, /)\n |      Implement iter(self).\n |  \n |  __le__(self, value, /)\n |      Return self&lt;=value.\n |  \n |  __len__(self, /)\n |      Return len(self).\n |  \n |  __lt__(self, value, /)\n |      Return self&lt;value.\n |  \n |  __mul__(self, value, /)\n |      Return self*value.\n |  \n |  __ne__(self, value, /)\n |      Return self!=value.\n |  \n |  __repr__(self, /)\n |      Return repr(self).\n |  \n |  __reversed__(self, /)\n |      Return a reverse iterator over the list.\n |  \n |  __rmul__(self, value, /)\n |      Return value*self.\n |  \n |  __setitem__(self, key, value, /)\n |      Set self[key] to value.\n |  \n |  __sizeof__(self, /)\n |      Return the size of the list in memory, in bytes.\n |  \n |  append(self, object, /)\n |      Append object to the end of the list.\n |  \n |  clear(self, /)\n |      Remove all items from list.\n |  \n |  copy(self, /)\n |      Return a shallow copy of the list.\n |  \n |  count(self, value, /)\n |      Return number of occurrences of value.\n |  \n |  extend(self, iterable, /)\n |      Extend list by appending elements from the iterable.\n |  \n |  index(self, value, start=0, stop=9223372036854775807, /)\n |      Return first index of value.\n |      \n |      Raises ValueError if the value is not present.\n |  \n |  insert(self, index, object, /)\n |      Insert object before index.\n |  \n |  pop(self, index=-1, /)\n |      Remove and return item at index (default last).\n |      \n |      Raises IndexError if list is empty or index is out of range.\n |  \n |  remove(self, value, /)\n |      Remove first occurrence of value.\n |      \n |      Raises ValueError if the value is not present.\n |  \n |  reverse(self, /)\n |      Reverse *IN PLACE*.\n |  \n |  sort(self, /, *, key=None, reverse=False)\n |      Sort the list in ascending order and return None.\n |      \n |      The sort is in-place (i.e. the list itself is modified) and stable (i.e. the\n |      order of two equal elements is maintained).\n |      \n |      If a key function is given, apply it once to each list item and sort them,\n |      ascending or descending, according to their function values.\n |      \n |      The reverse flag can be set to sort in descending order.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  __class_getitem__(...) from builtins.type\n |      See PEP 585\n |  \n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |  \n |  __new__(*args, **kwargs) from builtins.type\n |      Create and return a new object.  See help(type) for accurate signature.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __hash__ = None\n\n\n\nThat’s it for functions and methods. We will use them extensively in the upcoming chapters, so make sure you understand how they work.",
    "crumbs": [
      "The Very Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Here you can find a list of useful references that were used in the creation of this book.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n2023. An Introduction to Statistical Learning with Applications in\nPython. Springer. https://www.statlearning.com/.\n\n\nKaggle Datasets. 2024. “World Happiness Report - Dataset.”\nhttps://www.kaggle.com/datasets/unsdsn/world-happiness?select=2019.csv.\n\n\nMcKinney, Wes. 2022. Python for Data Analysis. O’Reilly Media.\nhttps://wesmckinney.com/book/.\n\n\nPython Software Foundation. 2024. “The Python Tutorial.” https://docs.python.org/3/tutorial/.",
    "crumbs": [
      "Wrapping up",
      "References"
    ]
  },
  {
    "objectID": "numpy.html",
    "href": "numpy.html",
    "title": "4  NumPy",
    "section": "",
    "text": "4.1 Installation and Loading\nYou can install Python packages using the pip command. Another popular package manager is conda. Either of these package managers will do the trick.\nOnce you have installed the NumPy package, you can load it using the following command.\nimport numpy as np\nWe have imported the NumPy package and aliased it as np. This is a common convention when working with NumPy. We can now use the functions and classes provided by NumPy by prefixing them with np.. For example, generating a random number using NumPy would look like this:\nnp.random.rand(5)\n\narray([0.97880901, 0.04743252, 0.94166398, 0.76965014, 0.43116348])",
    "crumbs": [
      "Basic Data Manipulation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#intallation-and-loading",
    "href": "numpy.html#intallation-and-loading",
    "title": "4  NumPy",
    "section": "",
    "text": "Tip\n\n\n\nYou can install NumPy using the following command:\npip install numpy",
    "crumbs": [
      "Basic Data Manipulation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#installation-and-loading",
    "href": "numpy.html#installation-and-loading",
    "title": "4  NumPy",
    "section": "",
    "text": "Tip\n\n\n\nInstalling NumPy using pip in rather simple. Just run the following command in your terminal or command prompt:\npip install numpy",
    "crumbs": [
      "Basic Data Manipulation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#what-can-numpy-do",
    "href": "numpy.html#what-can-numpy-do",
    "title": "4  NumPy",
    "section": "4.2 What can NumPy do?",
    "text": "4.2 What can NumPy do?\nOne of the key concepts in NumPy are array and matrix data structures. Moreover, Numpy provides tools for working with these structures. The array is in principle quite similar to a list in Python, with a few key differences:\n\nArrays can be multidimensional\nArrays can only contain elements of the same type, whereas lists can contain elements of different types\nArrays are optimized for numerical operations, whereas lists are not. This makes arrays much faster for numerical operations than lists, and also more memory efficient.",
    "crumbs": [
      "Basic Data Manipulation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#creating-numpy-arrays",
    "href": "numpy.html#creating-numpy-arrays",
    "title": "4  NumPy",
    "section": "4.3 Creating NumPy Arrays",
    "text": "4.3 Creating NumPy Arrays\n\n4.3.1 Creating 1-D NumPy Arrays\nYou can create a 1-D NumPy array from a list using the np.array() function. For example:\n\na = np.array([1, 2, 3, 4, 5])\nprint(a)\n\n[1 2 3 4 5]",
    "crumbs": [
      "Basic Data Manipulation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#numpy-arrays",
    "href": "numpy.html#numpy-arrays",
    "title": "4  NumPy",
    "section": "4.3 NumPy Arrays",
    "text": "4.3 NumPy Arrays\nYou can create a 1-D NumPy array from a list using the np.array() function. For example:\n\na = np.array([1, 2, 3, 4, 5])\na\n\narray([1, 2, 3, 4, 5])\n\n\nYou might have noticed that we actually used a list to create the NumPy array. We can naturally create a NumPy array from a list which has been assigned to a variable. For example, below we will cast a list to an array using the variable my_list:\n\nmy_list = [0, 1, 2, 3, 4, 5]\na = np.array(my_list)\nprint(a)\n\n[0 1 2 3 4 5]\n\n\nNow accessing the elements of the array is similar to accessing the elements of a list. For example, to access the first element of the array, you can use the following code:\n\na[0]\n\n0\n\n\nThis works just like it would for a list:\n\nmy_list[0]\n\n0\n\n\nYou can also use the slice notation to access a range of elements in the array. For example, the following code will access the elements from the second to the fourth element of the array. So in other words from index one to index three (it can be confusing I know).\n\na[1:4]\n\narray([1, 2, 3])\n\n\nWith arrays you can also do something called broadcasting. This means that you can apply an operation to every element in the array. For example, the following code will multiply every element in the array by 2.\n\na * 2\n\narray([ 0,  2,  4,  6,  8, 10])\n\n\nThis is not possible with lists as you will see by looking at the example below.\n\nmy_list * 2\n\n[0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5]\n\n\nYou can also apply mathematical functions to the array. For example, the following code will calculate the square of first three elements in the array.\n\nsquare_first_three = np.square(a[:3])\nprint(square_first_three)\n\n[0 1 4]\n\n\nYou should be aware that the array is not changed by these operations, as we can see by printing the array.\n\na\n\narray([0, 1, 2, 3, 4, 5])\n\n\n\n\n\n\n\n\nWarning\n\n\n\nHowever, when you broadcast on a slice of an array, the original array is changed. For example, the following code will change the first three elements of the array to their squares.\n\nsquared_slice = a[:3]\nsquared_slice **= 2\n\nprint(squared_slice)\nprint(a)\n\n[0 1 4]\n[0 1 4 3 4 5]\n\n\n\n\nYou can use the copy() method to create a copy of the array. This way you can change the copy without changing the original array. For example, the following code will create a copy of the array and change the copy without changing the original array.\n\na_copy = a.copy()\na_copy[0] = 100\nprint(a_copy)\nprint(a)\n\n[100   1   4   3   4   5]\n[0 1 4 3 4 5]\n\n\n\n4.3.1 Other Ways to Create 1-D Arrays\nnp.array() is not the only way to create a NumPy array. We can also create a one dimensional NumPy array for a range of numbers conveniently using the np.arange() function.\n\na = np.arange(1, 6)\nprint(a)\n\n[1 2 3 4 5]\n\n\nYou can also determine a step size for the range of numbers. For example, the following code will create an array with numbers from 0 to 10 with a step size of 2.\n\na_steps = np.arange(0, 11, 2)\nprint(a_steps)\n\n[ 0  2  4  6  8 10]\n\n\n\n\n4.3.2 2-D Matrices\nYou can create a 2-D NumPy array from a list of lists by using the np.array() function.\n\nb = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nb\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\nYou can access elements of a 2-D array using two indices. For example, the following code will access the element in the second row and third column of the array.\n\n# index for the second row and third column\nb[1, 2]\n\n6\n\n\nThere is also a double bracket notation for accessing elements in a 2-D array. For example, the following code will access the element in the second row and third column of the array.\n\nb[1][2]\n\n6\n\n\nEither of the two methods will work, but the first method should be more efficient.",
    "crumbs": [
      "Basic Data Manipulation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#some-convenient-functions",
    "href": "numpy.html#some-convenient-functions",
    "title": "4  NumPy",
    "section": "4.4 Some convenient functions",
    "text": "4.4 Some convenient functions\nHere are some convenient functions that you can use to create NumPy arrays:\n\nnp.zeros(): Creates an array of zeros\nnp.ones(): Creates an array of ones\nnp.linspace(): Creates an array of evenly spaced numbers over a specified range\nnp.eye(): Creates an identity matrix\n\nLet’s see some examples.\n\n4.4.1 Zeros and Ones\nYou can create an array of zeros using the np.zeros() function. For example, the following code will create an array of zeros with 5 elements.\n\nnp.zeros(5)\n\narray([0., 0., 0., 0., 0.])\n\n\nFor a 2-D array, you can specify the shape of the array as a tuple. For example, the following code will create a 2-D array of zeros with 3 rows and 4 columns.\n\nnp.zeros((3, 4))\n\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\n\n\nUsing ones is similar to using zeros. Let’s create a 5 by 6 matrix of ones.\n\nnp.ones((5, 6))\n\narray([[1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1.]])\n\n\n\n\n4.4.2 Linspace\nThe np.linspace() function is used to create an array of evenly spaced numbers over a specified range. For example, the following code will create an array of 10 numbers between 0 and 5.\n\nnp.linspace(0, 5, 10)\n\narray([0.        , 0.55555556, 1.11111111, 1.66666667, 2.22222222,\n       2.77777778, 3.33333333, 3.88888889, 4.44444444, 5.        ])\n\n\nHow does it differ from np.arange() you might ask? The np.linspace() function will always include the start and end values, whereas the np.arange() function will not include the end value.\n\n\n4.4.3 Eye\nThis function is used for creating an identity matrix. An identity matrix is a square matrix with ones on the diagonal and zeros elsewhere. We can create a 4 by 4 identity matrix with the following code.\n\nnp.eye(4)\n\narray([[1., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 0., 1.]])\n\n\nThe identity matrix has many important uses in linear algebra and other areas of mathematics.",
    "crumbs": [
      "Basic Data Manipulation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#array-operations",
    "href": "numpy.html#array-operations",
    "title": "4  NumPy",
    "section": "4.6 Array Operations",
    "text": "4.6 Array Operations\nYou can perform element-wise operations on NumPy arrays. For example, you can add two arrays together, subtract one array from another, multiply two arrays, and divide one array by another. Let’s see some examples.\n\n4.6.1 Basic Operations\nYou can perform basic arithmetic operations on NumPy arrays. For example, the following code will add two arrays together.\n\na = np.array([1, 2, 3, 4, 5])\n\na + a\n\narray([ 2,  4,  6,  8, 10])\n\n\nThe same goes for subtraction and multiplication.\n\n# subtraction\na - a\n\narray([0, 0, 0, 0, 0])\n\n\n\n# multiplication\na * a\n\narray([ 1,  4,  9, 16, 25])\n\n\nYou can also divide two arrays:\n\na / a\n\narray([1., 1., 1., 1., 1.])\n\n\nScalar operations are also possible. For example, the following code will multiply every element in the array by 2.\n\na * 2\n\narray([ 2,  4,  6,  8, 10])\n\n\nYou can also add or subtract a scalar from an array.\n\n# addition\na + 2\n\narray([3, 4, 5, 6, 7])\n\n\n\n\n4.6.2 Universal Functions\nNumPy provides a number of universal functions that can be applied to arrays. For example, the np.sqrt() function calculates the square root of every element in the array.\n\nnp.sqrt(a)\n\narray([1.        , 1.41421356, 1.73205081, 2.        , 2.23606798])\n\n\nYou can do things like finding the maximum or minimum value in an array.\n\n# maximum value\nnp.max(a)\n\n5\n\n\nThis is equivalent to using the max() method.\n\na.max()\n\n5\n\n\nYou can also find the index of the maximum value in the array.\n\nnp.argmax(a)\n\n4\n\n\nTrigonometric functions are also available, such as np.sin(), np.cos(), and np.tan().\n\nnp.cos(a)\n\narray([ 0.54030231, -0.41614684, -0.9899925 , -0.65364362,  0.28366219])\n\n\nThere are many more universal functions available in NumPy. You can find a list of them in the NumPy documentation. That’s it for NumPy. In the next section, we will look at Pandas, which will introduce us to DataFrames, a powerful data structure for data analysis in Python.",
    "crumbs": [
      "Basic Data Manipulation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#numpy-for-random-number-generation",
    "href": "numpy.html#numpy-for-random-number-generation",
    "title": "4  NumPy",
    "section": "4.5 NumPy for Random Number Generation",
    "text": "4.5 NumPy for Random Number Generation\nRandom numbers are needed for a variety of purposes in data analysis and machine learning. NumPy provides a number of functions for generating random numbers. Here are some of the most commonly used functions:\n\nnp.random.rand(): Generates random numbers from a uniform distribution\nnp.random.randn(): Generates random numbers from a standard normal distribution\nnp.random.randint(): Generates random integers\n\nThese functions allow us to create NumPy arrays with random numbers taken from different distributions.\n\n4.5.1 Uniform Distribution\nThe uniform distribution refers generally to random numbers between 0 and 1. The np.random.rand() function generates random numbers from a uniform distribution. For example, the following code will generate an array of 5 random numbers between 0 and 1.\n\nnp.random.rand(5)\n\narray([0.92448441, 0.90400502, 0.92213011, 0.92029374, 0.74739645])\n\n\nYou can also generate a 2-D array of random numbers. For example, the following code will generate a 3 by 4 array of random numbers.\n\narr = np.random.rand(3, 4)\narr\n\narray([[0.53633468, 0.80503591, 0.48212329, 0.49865715],\n       [0.4211912 , 0.69433433, 0.87418408, 0.23662667],\n       [0.14847428, 0.61232323, 0.6030776 , 0.458986  ]])\n\n\nWe can always check the shape of the array using the shape attribute.\n\narr.shape\n\n(3, 4)\n\n\nThe shape attribute returns a tuple with the dimensions of the array. In this case, the array has 3 rows and 4 columns. The function reshape() allows us to change the shape of the array. For example, we can reshape the array to have 4 rows and 3 columns, or to be one dimensional.\n\narr.reshape(4, 3)\n\narray([[0.53633468, 0.80503591, 0.48212329],\n       [0.49865715, 0.4211912 , 0.69433433],\n       [0.87418408, 0.23662667, 0.14847428],\n       [0.61232323, 0.6030776 , 0.458986  ]])\n\n\n\narr.reshape(12)\n\narray([0.53633468, 0.80503591, 0.48212329, 0.49865715, 0.4211912 ,\n       0.69433433, 0.87418408, 0.23662667, 0.14847428, 0.61232323,\n       0.6030776 , 0.458986  ])\n\n\nYou might have noticed, but the two dimensional array has two square brackets on the outer edges, whereas the one dimensional array has only one square bracket per side.\nFinally, we can also check the data type of the array using the dtype attribute. In case we want to change the data type of the array, we can use the astype() method.\n\narr.dtype\n\ndtype('float64')\n\n\n\narr.astype(int)\n\narray([[0, 0, 0, 0],\n       [0, 0, 0, 0],\n       [0, 0, 0, 0]])\n\n\n\n\n4.5.2 Normal Distribution\nThe normal distribution is quite possibly the most important distribution in statistics. The np.random.randn() function generates random numbers from a standard normal distribution. If we want to save ourselves some typing we can import the function directly.\n\nfrom numpy.random import randn\n\nThis allows us to use the function without the np. prefix. Like this:\n\n# create 5 random numbers from a standard normal distribution\nrandn(5)\n\narray([ 1.16773318, -1.03230255,  1.18685319,  0.27572565, -1.89376463])\n\n\nThe standard normal distribution is centered around zero and has a standard deviation of one. So, if we want to change the mean and standard deviation of our normal distribution, we can multiply the random numbers by the standard deviation and shift the mean by addition. For example, the following code will generate 5 random numbers from a normal distribution with a mean of 10 and a standard deviation of 2.\n\n# multiply by sd and add mean\nrandn(5) * 2 + 10\n\narray([10.73838116,  8.23491577,  9.79708733, 10.38771634,  9.32476956])\n\n\n\n\n4.5.3 Random Integers\nThe np.random.randint() function generates random integers. For example, the following code will generate eight random integers between 0 and 10.\n\nfrom numpy.random import randint\n\nrandint(0, 10, 8)\n\narray([7, 7, 9, 4, 0, 3, 3, 0])\n\n\nIf you want the results to be reproducible, you can set the seed using the np.random.seed() function. For example, the following code will generate the same random numbers every time you run it.\n\nnp.random.seed(42)\nrandint(0, 10, 8)\n\narray([6, 3, 7, 4, 6, 9, 2, 6])\n\n\nThe this code draws numbers with replacement from the integers between 0 and 10. If you want to draw numbers without replacement, you can use the choice() function. For example, the following code will draw 5 random numbers without replacement from the integers between 0 and 10.\n\nnp.random.choice(10, 5, replace=False)\n\narray([0, 6, 9, 1, 8])",
    "crumbs": [
      "Basic Data Manipulation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>NumPy</span>"
    ]
  },
  {
    "objectID": "pandas.html#series",
    "href": "pandas.html#series",
    "title": "5  Pandas Basics",
    "section": "",
    "text": "5.1.1 Creating a Series from other data types\nA List is not the only data type that can be used to create a series. You can also use a dictionary:\n\n# creating a series from a dictionary\nmy_dict = {'a': 1, 'b': 2, 'c': 3, 'd': 4}\npd.Series(my_dict)\n\na    1\nb    2\nc    3\nd    4\ndtype: int64\n\n\nYou can also create a series from a NumPy array etc.\n\n# creating a series from a NumPy array\nmy_array = np.array([1, 2, 3, 4])\npd.Series(my_array)\n\n0    1\n1    2\n2    3\n3    4\ndtype: int32\n\n\n\n\n5.1.2 Accessing elements of a Series\nWe already saw the bracket + index and the dot + index notation in action. In addition to using the index, we can use the iloc attribute to access the elements of the series by using numerical indexing:\n\n# access the first element of the series by using the positional index\nmy_series.iloc[0]\n\n1\n\n\nYou can also use the loc attribute to access the elements of the series by their index. You can add multiple index values to access multiple elements:\n\n# access the elements of the series by their index\nmy_series.loc[['a', 'c']]\n\na    1\nc    3\ndtype: int64\n\n\nWe can also use the : operator to access a range of elements:\n\n# access a range of elements\nmy_series.loc['a':'c']\n\na    1\nb    2\nc    3\ndtype: int64\n\n\nAssigning values to the elements of the series is also possible and works in the same way as with NumPy arrays:\n\n# assign a value to an element of the series\nmy_series['a'] = 100\nmy_series\n\na    100\nb      2\nc      3\nd      4\ndtype: int64",
    "crumbs": [
      "Basic Data Manipulation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Pandas Basics</span>"
    ]
  },
  {
    "objectID": "pandas.html#dataframe",
    "href": "pandas.html#dataframe",
    "title": "5  Pandas Basics",
    "section": "5.2 DataFrame",
    "text": "5.2 DataFrame\nA DataFrame is a two-dimensional, size-mutable, and heterogeneous tabular data structure with labeled axes (rows and columns). You can think of being similar to a spreadsheet. It is generally the most commonly used pandas object. Like the Series object we learned earlier, the DataFrame also accepts many different kinds of input. Let’s see what a DataFrame looks like:\n\n# create a DataFrame from a dictionary\ndata = {'name': ['John', 'Anna', 'Peter', 'Linda'],\n        'age': [23, 36, 32, 45],\n        'city': ['New York', 'Paris', 'Berlin', 'London']}\n\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\n\nname\nage\ncity\n\n\n\n\n0\nJohn\n23\nNew York\n\n\n1\nAnna\n36\nParis\n\n\n2\nPeter\n32\nBerlin\n\n\n3\nLinda\n45\nLondon\n\n\n\n\n\n\n\n\nWe can see that the DataFrame has a default index that starts from 0, and that the data is displayed in a tabular format which makes it easy to read. We can also specify the index values:\n\n# create a DataFrame with custom index\ndf = pd.DataFrame(data, index = ['a', 'b', 'c', 'd'])\ndf\n\n\n\n\n\n\n\n\n\nname\nage\ncity\n\n\n\n\na\nJohn\n23\nNew York\n\n\nb\nAnna\n36\nParis\n\n\nc\nPeter\n32\nBerlin\n\n\nd\nLinda\n45\nLondon\n\n\n\n\n\n\n\n\n\n5.2.1 Accessing Columns of a DataFrame\nThe columns are actually Pandas Series objects. We can access the columns of the DataFrame using the column name and the bracket notation. Let’s access the ‘name’ column of the DataFrame:\n\n# access the first column of the DataFrame\ndf['name']\n\na     John\nb     Anna\nc    Peter\nd    Linda\nName: name, dtype: object\n\n\nLet’s inspect the type of the column:\n\ntype(df['name'])\n\npandas.core.series.Series\n\n\nWe can see that the column is a Pandas Series object. We can also access the columns using the dot notation:\n\ndf.name\n\na     John\nb     Anna\nc    Peter\nd    Linda\nName: name, dtype: object\n\n\nThe issue that we may run into using the dot notation is that it may not work if the column we are trying to access has the same name as a DataFrame method. For example, if we have a column named ‘count’, we cannot access it using the dot notation because ‘count’ is a DataFrame method. We can access the columns using the loc attribute:\n\n# add count column to the DataFrame\ndf['count'] = [1, 2, 3, 4]\ndf\n\n\n\n\n\n\n\n\n\nname\nage\ncity\ncount\n\n\n\n\na\nJohn\n23\nNew York\n1\n\n\nb\nAnna\n36\nParis\n2\n\n\nc\nPeter\n32\nBerlin\n3\n\n\nd\nLinda\n45\nLondon\n4\n\n\n\n\n\n\n\n\nWe can access the columns using the loc attribute:\n\n# access the columns of the DataFrame using the loc attribute\ndf.loc[:, 'count']\n\na    1\nb    2\nc    3\nd    4\nName: count, dtype: int64\n\n\nAs we saw above, we can create a new column by assigning a list to a new column name. We can also create a new column by using the existing columns:\n\n# create a new column by using the existing columns\ndf['age_plus_count'] = df['age'] + df['count']\ndf\n\n\n\n\n\n\n\n\n\nname\nage\ncity\ncount\nage_plus_count\n\n\n\n\na\nJohn\n23\nNew York\n1\n24\n\n\nb\nAnna\n36\nParis\n2\n38\n\n\nc\nPeter\n32\nBerlin\n3\n35\n\n\nd\nLinda\n45\nLondon\n4\n49\n\n\n\n\n\n\n\n\nIf we now want to access all the new columns we created, we can use the bracket notation:\n\n# access the new columns\ndf[['count', 'age_plus_count']]\n\n\n\n\n\n\n\n\n\ncount\nage_plus_count\n\n\n\n\na\n1\n24\n\n\nb\n2\n38\n\n\nc\n3\n35\n\n\nd\n4\n49\n\n\n\n\n\n\n\n\nNow you might be wondering how to delete a column. We can use the drop method to delete a column:\n\n# deleting the new columns\ndf.drop(['count', 'age_plus_count'], axis=1)\n\n\n\n\n\n\n\n\n\nname\nage\ncity\n\n\n\n\na\nJohn\n23\nNew York\n\n\nb\nAnna\n36\nParis\n\n\nc\nPeter\n32\nBerlin\n\n\nd\nLinda\n45\nLondon\n\n\n\n\n\n\n\n\nThis prints out the DataFrame without the columns. However, you should note that the drop method does not modify the original DataFrame by default. We can confirm this by printing the original DataFrame:\n\ndf\n\n\n\n\n\n\n\n\n\nname\nage\ncity\ncount\nage_plus_count\n\n\n\n\na\nJohn\n23\nNew York\n1\n24\n\n\nb\nAnna\n36\nParis\n2\n38\n\n\nc\nPeter\n32\nBerlin\n3\n35\n\n\nd\nLinda\n45\nLondon\n4\n49\n\n\n\n\n\n\n\n\nIf you want to remove the columns permanently, you need to use the inplace parameter:\n\n# deleting the new columns permanently\ndf.drop(['count', 'age_plus_count'], axis=1, inplace=True)\ndf\n\n\n\n\n\n\n\n\n\nname\nage\ncity\n\n\n\n\na\nJohn\n23\nNew York\n\n\nb\nAnna\n36\nParis\n\n\nc\nPeter\n32\nBerlin\n\n\nd\nLinda\n45\nLondon\n\n\n\n\n\n\n\n\nThis gives us the original DataFrame without the columns we deleted. You might have noticed how we specified axis = 1 in the drop method. This is used to refer to dropping columns. Rows are dropped by specifying axis = 0. The terminology comes from NumPy, where the first axis is the rows and the second axis is the columns.\n\n\n5.2.2 Accessing Rows of a DataFrame\nOk, so we know how to access the columns of a DataFrame. How do we access the rows? There are a number of ways to do this. For example, we can use the loc attribute to access the rows of a DataFrame by their index:\n\n# access the rows of the DataFrame by their index\nrow_a = df.loc['a']\n\nrow_a\n\nname        John\nage           23\ncity    New York\nName: a, dtype: object\n\n\nIf we inspect the type of row_a, we can see that it is also a Pandas Series object, just like the individual columns were Series objects as well:\n\ntype(row_a)\n\npandas.core.series.Series\n\n\nHow do we access multiple rows? We can use the loc attribute and pass a list of index values:\n\n# access multiple rows\ndf.loc[['a', 'c']]\n\n\n\n\n\n\n\n\n\nname\nage\ncity\n\n\n\n\na\nJohn\n23\nNew York\n\n\nc\nPeter\n32\nBerlin\n\n\n\n\n\n\n\n\nThis, in turn, returns back a DataFrame. We can also use the iloc attribute to access the rows by their numerical index:\n\n# get the last two rows of the DataFrame\ndf.iloc[2:]\n\n\n\n\n\n\n\n\n\nname\nage\ncity\n\n\n\n\nc\nPeter\n32\nBerlin\n\n\nd\nLinda\n45\nLondon\n\n\n\n\n\n\n\n\nWe can also use the slicing notation for DataFrames. For example, to get the first two rows of the DataFrame:\n\ndf[:2]\n\n\n\n\n\n\n\n\n\nname\nage\ncity\n\n\n\n\na\nJohn\n23\nNew York\n\n\nb\nAnna\n36\nParis\n\n\n\n\n\n\n\n\nDo you still remember the drop method we used to delete columns? As we already discussed, we can also use it to delete rows. Let’s delete the first row:\n\n# delete the first row\ndf.drop('a', axis=0)\n\n\n\n\n\n\n\n\n\nname\nage\ncity\n\n\n\n\nb\nAnna\n36\nParis\n\n\nc\nPeter\n32\nBerlin\n\n\nd\nLinda\n45\nLondon\n\n\n\n\n\n\n\n\nSince we did not add inplace=True, the original DataFrame is not modified. We can confirm this by printing the original DataFrame:\n\ndf\n\n\n\n\n\n\n\n\n\nname\nage\ncity\n\n\n\n\na\nJohn\n23\nNew York\n\n\nb\nAnna\n36\nParis\n\n\nc\nPeter\n32\nBerlin\n\n\nd\nLinda\n45\nLondon\n\n\n\n\n\n\n\n\n\n\n5.2.3 Accessing Elements of a DataFrame\nWe are now familiar with some common ways of accessing specific rows and columns of a DataFrame. We are now ready to combine what we already know to access individual elements. We can use the loc attribute and pass the row and column index values:\n\n# access an element of the DataFrame\ndf.loc['a', 'name']\n\n'John'\n\n\nIf we know the numerical index of the row and column, we can use the iloc attribute:\n\n# get element from row 2 column 3\ndf.iloc[1, 2]\n\n'Paris'\n\n\nBy combining the techniques we learned, we can access multiple elements:\n\n# get multiple elements\ndf.loc[['a', 'c'], ['name', 'city']]\n\n\n\n\n\n\n\n\n\nname\ncity\n\n\n\n\na\nJohn\nNew York\n\n\nc\nPeter\nBerlin\n\n\n\n\n\n\n\n\n\n# first two rows\ndf[0:2][['name', 'age']]\n\n\n\n\n\n\n\n\n\nname\nage\n\n\n\n\na\nJohn\n23\n\n\nb\nAnna\n36",
    "crumbs": [
      "Basic Data Manipulation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Pandas Basics</span>"
    ]
  },
  {
    "objectID": "pandas.html#conditional-selection",
    "href": "pandas.html#conditional-selection",
    "title": "5  Pandas Basics",
    "section": "5.3 Conditional selection",
    "text": "5.3 Conditional selection\nIt quite common in data science to filter data based on some condition. With Pandas, we can use the bracket notation to filter our data. Let’s first create a DataFrame with random numbers so that we can better illustrate this point:\n\n# creating a numeric only DataFrame\nfrom numpy.random import randn\nnp.random.seed(101)\n\ndf2 = pd.DataFrame(randn(3, 4), index = ['A', 'B', 'C'], columns = ['col1', 'col2', 'col3', 'col4'])\n\ndf2\n\n\n\n\n\n\n\n\n\ncol1\ncol2\ncol3\ncol4\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\n\n\nC\n-2.018168\n0.740122\n0.528813\n-0.589001\n\n\n\n\n\n\n\n\nNow, we can see that our DataFrame contains a bunch of numbers. Let’s say we want to find all the elements that are greater than 0. We can use the bracket notation to create a condition that returns a boolean DataFrame:\n\ndf2 &gt; 0\n\n\n\n\n\n\n\n\n\ncol1\ncol2\ncol3\ncol4\n\n\n\n\nA\nTrue\nTrue\nTrue\nTrue\n\n\nB\nTrue\nFalse\nFalse\nTrue\n\n\nC\nFalse\nTrue\nTrue\nFalse\n\n\n\n\n\n\n\n\nAs a result, we get a DataFrame with equal dimensions to the original DataFrame, but the numeric values have been replaced with True or False values based on the condition. We can use this boolean DataFrame to filter the original DataFrame:\n\ndf2[df2 &gt; 0]\n\n\n\n\n\n\n\n\n\ncol1\ncol2\ncol3\ncol4\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\nB\n0.651118\nNaN\nNaN\n0.605965\n\n\nC\nNaN\n0.740122\n0.528813\nNaN\n\n\n\n\n\n\n\n\nNow we see that only the elements that satisfy the condition are displayed.\n\n5.3.1 Column based filtering\nIt is actually more common to select a subset of a DataFrame based on condition applied to a specific column. The idea is basically the same as we saw above. For example, let’s say we want to find all the elements in the col1 column that are greater than 0:\n\ndf2['col1'] &gt; 0\n\nA     True\nB     True\nC    False\nName: col1, dtype: bool\n\n\nThis returns a Pandas Series object with boolean values. We can use this Series object to filter the DataFrame:\n\ndf2[df2['col1'] &gt; 0]\n\n\n\n\n\n\n\n\n\ncol1\ncol2\ncol3\ncol4\n\n\n\n\nA\n2.706850\n0.628133\n0.907969\n0.503826\n\n\nB\n0.651118\n-0.319318\n-0.848077\n0.605965\n\n\n\n\n\n\n\n\nSimilarly, if we go back to our original DataFrame with the people data, we could filter the DataFrame based on the age column. Let’s say we want to find people over the age of 25:\n\ndf[df['age'] &gt; 25]\n\n\n\n\n\n\n\n\n\nname\nage\ncity\n\n\n\n\nb\nAnna\n36\nParis\n\n\nc\nPeter\n32\nBerlin\n\n\nd\nLinda\n45\nLondon\n\n\n\n\n\n\n\n\nVery convenient.\n\n\n5.3.2 Multiple conditions\nFiltering using multiple conditions is also possible. We can use the & (and) operator to combine the conditions. Let’s say we want to find people over the age of 25 who live in Paris:\n\ndf[(df['age'] &gt; 25) & (df['city'] == 'Paris')]\n\n\n\n\n\n\n\n\n\nname\nage\ncity\n\n\n\n\nb\nAnna\n36\nParis\n\n\n\n\n\n\n\n\nPlease note the syntax: we need to use parentheses around each condition. Also, we are not using the and keyword. We are using the & operator. This brings us to the next point: we can use the | (or) operator to combine conditions. Let’s say we want to find people younger than 25 or people who live in Paris:\n\ndf[(df['age'] &lt; 25) | (df['city'] == 'Paris')]\n\n\n\n\n\n\n\n\n\nname\nage\ncity\n\n\n\n\na\nJohn\n23\nNew York\n\n\nb\nAnna\n36\nParis\n\n\n\n\n\n\n\n\nPretty neat. We can also use the ~ (not) operator to negate a condition. Let’s say we want to find people who do not live in Paris:\n\ndf[~(df['city'] == 'Paris')]\n\n\n\n\n\n\n\n\n\nname\nage\ncity\n\n\n\n\na\nJohn\n23\nNew York\n\n\nc\nPeter\n32\nBerlin\n\n\nd\nLinda\n45\nLondon\n\n\n\n\n\n\n\n\nSince the result is also a DataFrame, we can interrogate the results as we would with any other DataFrame. For example, getting the names of people who do not live in Paris is easy:\n\ndf[~(df['city'] == 'Paris')]['name']\n\na     John\nc    Peter\nd    Linda\nName: name, dtype: object",
    "crumbs": [
      "Basic Data Manipulation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Pandas Basics</span>"
    ]
  },
  {
    "objectID": "pandas.html#summary",
    "href": "pandas.html#summary",
    "title": "5  Pandas Basics",
    "section": "5.4 Summary",
    "text": "5.4 Summary\nIn this chapter, we learned about the basics of the Pandas library. In the next chapter, we will learn a little bit more about working with Pandas DataFrames, including how to handle missing data and how to group data.",
    "crumbs": [
      "Basic Data Manipulation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Pandas Basics</span>"
    ]
  },
  {
    "objectID": "pandas_advanced.html",
    "href": "pandas_advanced.html",
    "title": "6  Working with DataFrames",
    "section": "",
    "text": "6.1 Common methods in Pandas\nWe already saw that the head() method displays the first five rows of the dataset. Five is the default value, but we can change in using the n parameter. Similarly, we can use the tail() method to display the last rows of the dataset.\n# Display the last nine rows\ndf.tail(9)\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n141\n6.9\n3.1\n5.1\n2.3\nvirginica\n\n\n142\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n143\n6.8\n3.2\n5.9\n2.3\nvirginica\n\n\n144\n6.7\n3.3\n5.7\n2.5\nvirginica\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica",
    "crumbs": [
      "Basic Data Manipulation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with DataFrames</span>"
    ]
  },
  {
    "objectID": "pandas_advanced.html#grouping-data",
    "href": "pandas_advanced.html#grouping-data",
    "title": "6  Working with DataFrames",
    "section": "6.2 Grouping data",
    "text": "6.2 Grouping data\nIf you are familiar with SQL queries, you might have used the GROUP BY clause to group data based on a particular column. Pandas provides a similar functionality using the groupby() method. Grouping data allows you to calculate for example statistics like the mean for distinct groups in the dataset. Let’s take a look at how this works by grouping the iris data based on the species column and calculating the mean of the other columns.\n\ndf.groupby('species').mean()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\nspecies\n\n\n\n\n\n\n\n\nsetosa\n5.006\n3.428\n1.462\n0.246\n\n\nversicolor\n5.936\n2.770\n4.260\n1.326\n\n\nvirginica\n6.588\n2.974\n5.552\n2.026\n\n\n\n\n\n\n\n\nWe can also target specific columns. Let’s see how this works by taking the sum of the sepal_length for each species.\n\ndf.groupby('species')['sepal_length'].sum()\n\nspecies\nsetosa        250.3\nversicolor    296.8\nvirginica     329.4\nName: sepal_length, dtype: float64\n\n\nSome commonly used aggregation functions to chain after a groupby clause are:\n\nmean()\nsum()\ncount()\nmin()\nmax()\n\nWe can also use the agg() method to apply multiple aggregation functions at once. Let’s by taking the count, min, and max of the sepal_length and sepal_width columns for each species.\n\ndf.groupby('species')[['sepal_length', 'sepal_width']].agg(['count', 'min', 'max'])\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\n\n\n\ncount\nmin\nmax\ncount\nmin\nmax\n\n\nspecies\n\n\n\n\n\n\n\n\n\n\nsetosa\n50\n4.3\n5.8\n50\n2.3\n4.4\n\n\nversicolor\n50\n4.9\n7.0\n50\n2.0\n3.4\n\n\nvirginica\n50\n4.9\n7.9\n50\n2.2\n3.8\n\n\n\n\n\n\n\n\nIf you want to get summary statistics per group, you can also use the describe() method.\n\n# let's get the summary statistics for all columns\n# and show results for the petal_length column\ndf.groupby('species').describe()['petal_length']\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nspecies\n\n\n\n\n\n\n\n\n\n\n\n\nsetosa\n50.0\n1.462\n0.173664\n1.0\n1.4\n1.50\n1.575\n1.9\n\n\nversicolor\n50.0\n4.260\n0.469911\n3.0\n4.0\n4.35\n4.600\n5.1\n\n\nvirginica\n50.0\n5.552\n0.551895\n4.5\n5.1\n5.55\n5.875\n6.9",
    "crumbs": [
      "Basic Data Manipulation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with DataFrames</span>"
    ]
  },
  {
    "objectID": "pandas_advanced.html#handling-missing-values",
    "href": "pandas_advanced.html#handling-missing-values",
    "title": "6  Working with DataFrames",
    "section": "6.3 Handling missing values",
    "text": "6.3 Handling missing values\nIn a perfect world, we would always work with complete data tables without any missing values. However, out in the wild we are often confronted with incomplete data, which is why dealing with missing values is an important skill to have. Pandas provides a few methods to handle missing values.\nLet’s look at our iris dataset and introduce some missing values.\n\n# one missing value to the sepal_length column row 3\ndf.loc[2, 'sepal_length'] = np.nan\ndf.head()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\nNaN\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\nThere are a couple of methods which help us determine if missing values are present. The following methods are commonly used:\n\nisnull(): returns a DataFrame of the same shape as the original dataset, where each cell is either True or False depending on whether the value is missing or not.\nnotnull(): returns the opposite of isnull(). It returns True if the value is not missing, and False otherwise.\nisna(): an alias for isnull().\nnotna(): an alias for notnull().\n\nThe isnull() method returns a DataFrame of the same shape as the original dataset, where each cell is either True or False depending on whether the value is missing or not.\n\ndf.head().isnull()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\n\nThe isna() method does exactly the same thing as isnull(), which we can see in the example below.\n\ndf.head().isna()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n3\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\n\nThe notnull() method returns the opposite of isnull(). It returns True if the value is not missing, and False otherwise.\n\ndf.head()['sepal_length'].notnull()\n\n0     True\n1     True\n2    False\n3     True\n4     True\nName: sepal_length, dtype: bool\n\n\nRelated to missing values we can easily drop rows with missing values using the dropna() method. By default, it drops rows where at least one element is missing.\n\ndf.dropna().head()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\n\n\n\nThe dropna() method also has a parameter how which can be set to all. This will only drop rows where all elements are missing. It is also possible to drop columns with missing values by setting the axis parameter to 1. Let’s see how this works.\n\n# drop columns with missing values\ndf.dropna(axis=1).head()\n\n\n\n\n\n\n\n\n\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\n\n6.3.1 Filling missing values\nInstead of dropping rows with missing values, we can also fill them with a specific value. This is ofter referred to as imputation. The fillna() method allows us to fill missing values with a specific value. Let’s fill the missing values in the sepal_length column with the mean of the column.\n\n# use the mean of sepal_length column to fill in for missing values\ndf['sepal_length'] = df['sepal_length'].fillna(df['sepal_length'].mean())\ndf.head()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.100000\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.900000\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n5.851007\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.600000\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.000000\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\n\n\n6.3.2 Missing values for other data types\nSo far, we have only looked at missing values in numerical columns. However, missing values can also occur in categorical columns. Let’s introduce a missing value in the species column.\n\n# introduce a missing value in the species column\ndf.loc[2, 'species'] = np.nan\ndf.head()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.100000\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.900000\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n5.851007\n3.2\n1.3\n0.2\nNaN\n\n\n3\n4.600000\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.000000\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\nLet’s inspect the datatypes of the columns in the dataset.\n\ndf.dtypes\n\nsepal_length    float64\nsepal_width     float64\npetal_length    float64\npetal_width     float64\nspecies          object\ndtype: object\n\n\nWe can now try to impute the most common value in the species column for the missing value. Before we do that let’s show to use the mode() method to get the most common value in a column.\n\ndf['species'].mode()\n\n0    versicolor\n1     virginica\nName: species, dtype: object\n\n\nWe can see that mode returns a Series object. To get the actual value we can use the iloc method, or just use the mode()[0] method. Let’s use this to fill the missing value in the species column.\n\n# fill missing values in the species column with the most common value\ndf['species'] = df['species'].fillna(df['species'].mode()[0])\ndf.head()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.100000\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.900000\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n5.851007\n3.2\n1.3\n0.2\nversicolor\n\n\n3\n4.600000\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.000000\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\nObviously, here we have imputed the incorrect species in place of the missing value. In practice, you would need to be more careful and consider the context of the data before imputing missing values. However, this example shows how you can impute missing values in categorical columns. There is more to learn about missing values, but for now, we will leave it at this.",
    "crumbs": [
      "Basic Data Manipulation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with DataFrames</span>"
    ]
  },
  {
    "objectID": "pandas_advanced.html#joining-dataframes",
    "href": "pandas_advanced.html#joining-dataframes",
    "title": "6  Working with DataFrames",
    "section": "6.4 Joining DataFrames",
    "text": "6.4 Joining DataFrames\nQuite often we find ourselves working with multiple datasets that we need to combine. Pandas provides a few methods to join DataFrames. Let’s start by creating two DataFrames to demonstrate how this works.\n\nsales_regions = pd.DataFrame({\n    'region': ['North', 'South', 'East', 'West'],\n    'manager': ['John', 'Sara', 'Tom', 'Alice']\n})\n\nsales_regions\n\n\n\n\n\n\n\n\n\nregion\nmanager\n\n\n\n\n0\nNorth\nJohn\n\n\n1\nSouth\nSara\n\n\n2\nEast\nTom\n\n\n3\nWest\nAlice\n\n\n\n\n\n\n\n\n\n# sales for three years by region\nsales_results = pd.DataFrame({\n    'year': [2019, 2020, 2021, 2019, 2020, 2021, \n    2019, 2020, 2021, 2019, 2020, 2021],\n    'region': ['North', 'North', 'North', 'South', 'South', 'South', \n    'East', 'East', 'East', 'West', 'West', 'West'],\n    'sales': [1000, 1200, 1500, 800, 900, 1000, \n    700, 800, 900, 600, 700, 800]\n})\n\nsales_results\n\n\n\n\n\n\n\n\n\nyear\nregion\nsales\n\n\n\n\n0\n2019\nNorth\n1000\n\n\n1\n2020\nNorth\n1200\n\n\n2\n2021\nNorth\n1500\n\n\n3\n2019\nSouth\n800\n\n\n4\n2020\nSouth\n900\n\n\n5\n2021\nSouth\n1000\n\n\n6\n2019\nEast\n700\n\n\n7\n2020\nEast\n800\n\n\n8\n2021\nEast\n900\n\n\n9\n2019\nWest\n600\n\n\n10\n2020\nWest\n700\n\n\n11\n2021\nWest\n800\n\n\n\n\n\n\n\n\nGiven these DataFrames, might want to join them based on the region column. We could for example calculate the total sales per region, and add the information to the sales_regions DataFrame. We can do this using the merge() method. Let’s see how this works.\n\n# let's first calculate the total sales per region\nsales_per_region = sales_results.groupby('region')['sales'].sum().reset_index()\nsales_per_region\n\n\n\n\n\n\n\n\n\nregion\nsales\n\n\n\n\n0\nEast\n2400\n\n\n1\nNorth\n3700\n\n\n2\nSouth\n2700\n\n\n3\nWest\n2100\n\n\n\n\n\n\n\n\nWe used the groupby() method we learned earlier to calculate the total sales per region. We then used the reset_index() method to convert the grouped data back to a DataFrame. Now we can merge the sales_regions DataFrame with the sales_per_region DataFrame.\n\n# merge the sales_regions DataFrame with the sales_per_region DataFrame\nsales_regions.merge(sales_per_region, on='region', how='left')\n\n\n\n\n\n\n\n\n\nregion\nmanager\nsales\n\n\n\n\n0\nNorth\nJohn\n3700\n\n\n1\nSouth\nSara\n2700\n\n\n2\nEast\nTom\n2400\n\n\n3\nWest\nAlice\n2100\n\n\n\n\n\n\n\n\nIf you have ever used SQL, you might have noticed that this feels very similar to a SQL join. The on parameter specifies the column to join on, and the how parameter specifies the type of join. Most common choices for the how parameter are probably left, right, inner, and outer. The default value is inner.\nThere is more functionality when it comes using merge for joining DataFrames, but for the time being we will leave it at this.\n\n6.4.1 Concatenating DataFrames\nNow sometimes we have DataFrames where the content is different but the columns are the same. In this case we might want to concatenate the DataFrames. Let’s assume we have sales data from previous years that we want to add to the sales_results DataFrame.\n\nsales_results_2018 = pd.DataFrame({\n    'year': [2018, 2018, 2018, 2018],\n    'region': ['North', 'South', 'East', 'West'],\n    'sales': [500, 400, 300, 200]\n})\n\nsales_results_2018\n\n\n\n\n\n\n\n\n\nyear\nregion\nsales\n\n\n\n\n0\n2018\nNorth\n500\n\n\n1\n2018\nSouth\n400\n\n\n2\n2018\nEast\n300\n\n\n3\n2018\nWest\n200\n\n\n\n\n\n\n\n\nWe can see that the format of this DataFrame is the same as the sales_results DataFrame. We can concatenate the two DataFrames using the concat() method, which takes a list of DataFrames as input.\n\n# concatenate the sales results from different years\nsales_all = pd.concat([sales_results_2018, sales_results])\nsales_all\n\n\n\n\n\n\n\n\n\nyear\nregion\nsales\n\n\n\n\n0\n2018\nNorth\n500\n\n\n1\n2018\nSouth\n400\n\n\n2\n2018\nEast\n300\n\n\n3\n2018\nWest\n200\n\n\n0\n2019\nNorth\n1000\n\n\n1\n2020\nNorth\n1200\n\n\n2\n2021\nNorth\n1500\n\n\n3\n2019\nSouth\n800\n\n\n4\n2020\nSouth\n900\n\n\n5\n2021\nSouth\n1000\n\n\n6\n2019\nEast\n700\n\n\n7\n2020\nEast\n800\n\n\n8\n2021\nEast\n900\n\n\n9\n2019\nWest\n600\n\n\n10\n2020\nWest\n700\n\n\n11\n2021\nWest\n800\n\n\n\n\n\n\n\n\nAll looks good, but we can see that the index is not in order. We can reset the index using the reset_index() method.\n\nsales_all.reset_index(drop=True, inplace=True)\nsales_all\n\n\n\n\n\n\n\n\n\nyear\nregion\nsales\n\n\n\n\n0\n2018\nNorth\n500\n\n\n1\n2018\nSouth\n400\n\n\n2\n2018\nEast\n300\n\n\n3\n2018\nWest\n200\n\n\n4\n2019\nNorth\n1000\n\n\n5\n2020\nNorth\n1200\n\n\n6\n2021\nNorth\n1500\n\n\n7\n2019\nSouth\n800\n\n\n8\n2020\nSouth\n900\n\n\n9\n2021\nSouth\n1000\n\n\n10\n2019\nEast\n700\n\n\n11\n2020\nEast\n800\n\n\n12\n2021\nEast\n900\n\n\n13\n2019\nWest\n600\n\n\n14\n2020\nWest\n700\n\n\n15\n2021\nWest\n800\n\n\n\n\n\n\n\n\nThe drop=True parameter means that we want to get rid of the old index. The inplace=True parameter means that we want to modify the DataFrame in place, instead of creating a new DataFrame. This will make the index start from 0, and the changes are applied to the DataFrame.\n\n\n6.4.2 Concatenating along columns\nSomethis we want to stick new columns at the end of an existing DataFrame. We can do this using the concat() method with the axis parameter set to 1. Let’s look at an example.\n\nsales_by_year = pd.DataFrame({\n    'region': ['North', 'South', 'East', 'West'],\n    '2018': [500, 400, 300, 200],   \n    '2019': [1000, 800, 700, 600],\n    '2020': [1200, 900, 800, 700]\n})\n\nsales_by_year\n\n\n\n\n\n\n\n\n\nregion\n2018\n2019\n2020\n\n\n\n\n0\nNorth\n500\n1000\n1200\n\n\n1\nSouth\n400\n800\n900\n\n\n2\nEast\n300\n700\n800\n\n\n3\nWest\n200\n600\n700\n\n\n\n\n\n\n\n\n\nyear_2021_2022 = pd.DataFrame({\n    '2021': [1500, 1000, 900, 800],\n    '2022': [1600, 1100, 1000, 900]\n})\n\nyear_2021_2022\n\n\n\n\n\n\n\n\n\n2021\n2022\n\n\n\n\n0\n1500\n1600\n\n\n1\n1000\n1100\n\n\n2\n900\n1000\n\n\n3\n800\n900\n\n\n\n\n\n\n\n\nWe can see that the year_2021_2022 DataFrame is a natural extension to the sales_by_year DataFrame. Let’s combine the two.\n\nsales_by_year = pd.concat([sales_by_year, year_2021_2022], axis=1)\nsales_by_year\n\n\n\n\n\n\n\n\n\nregion\n2018\n2019\n2020\n2021\n2022\n\n\n\n\n0\nNorth\n500\n1000\n1200\n1500\n1600\n\n\n1\nSouth\n400\n800\n900\n1000\n1100\n\n\n2\nEast\n300\n700\n800\n900\n1000\n\n\n3\nWest\n200\n600\n700\n800\n900\n\n\n\n\n\n\n\n\n\n\n6.4.3 Joining DataFrames on the index values\nSo far we have joined DataFrames based on columns. However, we can also join DataFrames based on the index values. Let’s see how this works by creating two DataFrames with the some overlapping index values.\n\ndf1 = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4, 5, 6]},\n    index=[1, 2, 3])\ndf1\n\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n1\n1\n4\n\n\n2\n2\n5\n\n\n3\n3\n6\n\n\n\n\n\n\n\n\n\ndf2 = pd.DataFrame({\n    'C': [7, 8, 9],\n    'D': [10, 11, 12]},\n    index=[2, 3, 4])\n\ndf2\n\n\n\n\n\n\n\n\n\nC\nD\n\n\n\n\n2\n7\n10\n\n\n3\n8\n11\n\n\n4\n9\n12\n\n\n\n\n\n\n\n\nWe can now join the two DataFrames based on the index values using the join() method. Let’s take a look at a left join.\n\ndf1.join(df2, how='left')\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n1\n1\n4\nNaN\nNaN\n\n\n2\n2\n5\n7.0\n10.0\n\n\n3\n3\n6\n8.0\n11.0\n\n\n\n\n\n\n\n\nWe can see that the df1 DataFrame is the left DataFrame, which will be preserved in the join operation. The df2 DataFrame is the right DataFrame, which will be joined to the left DataFrame based on the index values. If a value is missing in the right DataFrame, it will be filled with NaN.\nA right join will preserve the right DataFrame.\n\ndf1.join(df2, how='right')\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n2\n2.0\n5.0\n7\n10\n\n\n3\n3.0\n6.0\n8\n11\n\n\n4\nNaN\nNaN\n9\n12",
    "crumbs": [
      "Basic Data Manipulation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with DataFrames</span>"
    ]
  },
  {
    "objectID": "pandas_advanced.html#common-methods-in-pandas",
    "href": "pandas_advanced.html#common-methods-in-pandas",
    "title": "6  Working with DataFrames",
    "section": "",
    "text": "6.1.1 General information about the dataset\nThe info() method provides a concise summary of the dataset. It displays the number of non-null values in each column, the data type of each column, and the memory usage of the dataset.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  150 non-null    float64\n 1   sepal_width   150 non-null    float64\n 2   petal_length  150 non-null    float64\n 3   petal_width   150 non-null    float64\n 4   species       150 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 6.0+ KB\n\n\nSummary statistics can be obtained using the describe() method. It provides the count, mean, standard deviation, minimum, maximum, and the quartiles of the dataset. This provides a handy way of getting an overall impression of the dataset.\n\ndf.describe()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\ncount\n150.000000\n150.000000\n150.000000\n150.000000\n\n\nmean\n5.843333\n3.057333\n3.758000\n1.199333\n\n\nstd\n0.828066\n0.435866\n1.765298\n0.762238\n\n\nmin\n4.300000\n2.000000\n1.000000\n0.100000\n\n\n25%\n5.100000\n2.800000\n1.600000\n0.300000\n\n\n50%\n5.800000\n3.000000\n4.350000\n1.300000\n\n\n75%\n6.400000\n3.300000\n5.100000\n1.800000\n\n\nmax\n7.900000\n4.400000\n6.900000\n2.500000\n\n\n\n\n\n\n\n\nIf we just want to know the number of rows and columns in the dataset, we can use the shape attribute.\n\ndf.shape\n\n(150, 5)\n\n\nThe columns attribute provides the names of the columns in the dataset.\n\ndf.columns\n\nIndex(['sepal_length', 'sepal_width', 'petal_length', 'petal_width',\n       'species'],\n      dtype='object')\n\n\n\n\n6.1.2 Unique values in the dataset\nThere are a few methods that help us get unique values in the dataset. The unique() method provides the unique values in a column. Let’s see how this works by getting the unique values in the species column.\n\nunique_species = df['species'].unique()\nunique_species\n\narray(['setosa', 'versicolor', 'virginica'], dtype=object)\n\n\nWe can see that the dataset contains three unique species of iris flowers. This is pretty simple to see directly, but if we would have more than three species, we could also check the number of unique values like this:\n\nlen(unique_species)\n\n3\n\n\nThere is actually a dedicated method for obtaining the number of unique values. The nunique() method provides the number of unique values in each column of the dataset.\n\ndf.nunique()\n\nsepal_length    35\nsepal_width     23\npetal_length    43\npetal_width     22\nspecies          3\ndtype: int64\n\n\nFinally, the value_counts() method provides the frequency of each unique value in a column. Let’s see how this works by getting the frequency of each species in the species column.\n\ndf['species'].value_counts()\n\nspecies\nsetosa        50\nversicolor    50\nvirginica     50\nName: count, dtype: int64\n\n\n\n\n6.1.3 Sorting values\nThe sort_values() method allows us to sort the dataset based on one or more columns. Let’s see how this works by sorting the dataset based on the sepal_length column.\n\ndf.sort_values('sepal_length').head()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n13\n4.3\n3.0\n1.1\n0.1\nsetosa\n\n\n42\n4.4\n3.2\n1.3\n0.2\nsetosa\n\n\n38\n4.4\n3.0\n1.3\n0.2\nsetosa\n\n\n8\n4.4\n2.9\n1.4\n0.2\nsetosa\n\n\n41\n4.5\n2.3\n1.3\n0.3\nsetosa\n\n\n\n\n\n\n\n\n\n\n6.1.4 Applying functions to columns\nThe apply() method allows us to apply a function to columns in the dataset. For example, if we take a look at the first few values for the species column, we can see that the values are strings. Let’s apply the upper() method to the species column to convert the strings to uppercase.\n\ndf['species'].apply(lambda x: x.upper()).head()\n\n0    SETOSA\n1    SETOSA\n2    SETOSA\n3    SETOSA\n4    SETOSA\nName: species, dtype: object\n\n\nFor simple functions like finding the length of a string, we can use the len function directly.\n\ndf['species'].apply(len).head()\n\n0    6\n1    6\n2    6\n3    6\n4    6\nName: species, dtype: int64\n\n\nThe apply() method coupled with lambda expressions grants us the ability to easily craft custom modifications on existing columns. This will be useful when we need to clean or transform data.\n\n# Make species uppercase and add a suffix to the end\ndf['species'].apply(lambda x: x.upper() + '_SPECIES').head()\n\n0    SETOSA_SPECIES\n1    SETOSA_SPECIES\n2    SETOSA_SPECIES\n3    SETOSA_SPECIES\n4    SETOSA_SPECIES\nName: species, dtype: object",
    "crumbs": [
      "Basic Data Manipulation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with DataFrames</span>"
    ]
  },
  {
    "objectID": "pandas_advanced.html#reading-in-data",
    "href": "pandas_advanced.html#reading-in-data",
    "title": "6  Working with DataFrames",
    "section": "6.5 Reading in data",
    "text": "6.5 Reading in data\nIn the real world, we aren’t usually working with example datasets like the iris dataset. Instead, we are burdened with the task of reading in the data. Luckily, Pandas provides a variety of methods to read in data from different sources. Let’s take a look at probably the most common method: read_csv(). As the name suggests, this method reads in data from a CSV file. We are going to read in a dataset called pets, which is saved as a CSV file under a folder called data.\n\n# read in the pets dataset\npets = pd.read_csv('data/pets.csv')\npets\n\n\n\n\n\n\n\n\n\nPet\nName\nAge\nOwner\n\n\n\n\n0\nParrot\nPolly\n15\nCathy\n\n\n1\nCat\nFluffy\n3\nJohn\n\n\n2\nDog\nSpot\n5\nAmanda\n\n\n3\nGuinea pig\nSqueeky\n1\nMark\n\n\n\n\n\n\n\n\nWe can see that the read_csv() method reads in the data from the CSV file and creates a DataFrame. As mentioned earlier, Pandas provides several methods for reading in many file types such as Excel files, SQL databases, and more. You can browse the available methods by typing pd.read_ and pressing Tab for autocomplete suggestions. For saving data to a file you can use the to_csv() method. Let’s save the pets DataFrame to a CSV file.\n# save the pets DataFrame to a CSV file\npets.to_csv('data/pets_saved.csv', index=False)\nHere we used the index=False parameter to avoid saving the index values to the CSV file. This is usually preferred, unless you have a specific reason for creating an extra column for the index values in the saved file. Next we will turn our attention to visualizing data, which is one of the most important aspects of data analysis.",
    "crumbs": [
      "Basic Data Manipulation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Working with DataFrames</span>"
    ]
  },
  {
    "objectID": "matplotlib.html",
    "href": "matplotlib.html",
    "title": "7  Data visualisation with Matplotlib",
    "section": "",
    "text": "7.1 Getting started\nLet’s start by creating a simple line plot using Matplotlib. The following code generates data along a sine wave and plots it using Matplotlib. We start by importing the necessary libraries.\nimport matplotlib.pyplot as plt\nimport numpy as np\nNext, we generate the data and plot it using Matplotlib (Figure 7.1).\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n\n\nFigure 7.1: Sine wave plot created using Matplotlib.\nThe plt.plot() function is used to create the plot, and the plt.show() function is used to display the plot. The x and y arrays contain the data for the x-axis and y-axis, respectively. Essentially, we were able to create a decent looking plot with just four lines of code.\nLet’s customize the plot a bit by adding labels to the x-axis and y-axis, a title, and a grid.\nplt.plot(x, y)\nplt.xlabel('x axis')\nplt.ylabel('y axis')\nplt.title('Sine wave plot')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 7.2: Sine wave plot with axis labels, title, and grid.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data visualisation with Matplotlib</span>"
    ]
  },
  {
    "objectID": "matplotlib.html#getting-started",
    "href": "matplotlib.html#getting-started",
    "title": "7  Data visualisation with Matplotlib",
    "section": "",
    "text": "7.1.1 The second way of creating a plot\nWhat makes Matplotlib a bit tricky for beginners is that it has two interfaces: and explicit and implicit one. The one we saw above is the implicit interface, which resembles MATLAB, and makes it easier to create simple plots. The explicit interface is more powerful and flexible, but it requires more lines of code. Let’s see how we can create the same plot using the Object Oriented (OO) explicit interface.\n\nfig, ax = plt.subplots()\nax.plot(x, y)\nplt.show()\n\n\n\n\n\n\n\nFigure 7.3: Sine wave plot using the Object Oriented (OO) explicit interface.\n\n\n\n\n\nIn the second approach, we first create a figure and an axis object using plt.subplots(). We then use the plot() method of the axis object to create the plot. In a simple case like this one, the difference between the two interfaces isn’t too big.\nLet’s add the missing labels, title, and grid to the plot.\n\nfig, ax = plt.subplots()\nax.plot(x, y)\nax.set_xlabel('x axis')\nax.set_ylabel('y axis')\nax.set_title('Sine wave plot')\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\nFigure 7.4: Sine wave plot using the Object Oriented (OO) explicit interface with axis labels, title, and grid.\n\n\n\n\n\nNow we have a plot that is identical to the one we created earlier. It’s fine to use either of the two approaches, but the latter (object oriented) is recommended by the Matplotlib developers (especially for more elaborate plots).",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data visualisation with Matplotlib</span>"
    ]
  },
  {
    "objectID": "matplotlib.html#plotting-pandas-data",
    "href": "matplotlib.html#plotting-pandas-data",
    "title": "7  Data visualisation with Matplotlib",
    "section": "7.2 Plotting Pandas data",
    "text": "7.2 Plotting Pandas data\nMatplotlib can be used to plot data stored in Pandas data structures. Let’s create a simple line plot using a Pandas DataFrame. We start by importing the necessary libraries and the iris dataset (Table 7.1).\n\nimport pandas as pd\nimport seaborn as sns\n\n# Load the dataset\ndf = sns.load_dataset('iris')\ndf.head()\n\n\n\nTable 7.1: The first few rows of the iris dataset.\n\n\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\n\n\n\n\nLet’s start by creating a simple scatter plot using the iris dataset. We will plot the sepal length against the sepal width.\n\nfig, ax = plt.subplots()\nax.scatter(df['sepal_length'], df['sepal_width'])\nax.set_xlabel('Sepal length')\nax.set_ylabel('Sepal width')\nax.set_title('Sepal length vs Sepal width')\nplt.show()\n\n\n\n\n\n\n\nFigure 7.5: Scatter plot of sepal length vs sepal width using the iris dataset.\n\n\n\n\n\nThe scatter() method is used to create a scatter plot. We pass the sepal length and sepal width columns from the iris dataset to the scatter() method. We then add labels to the x-axis and y-axis, a title, and display the plot.\nNow, we can make the plot more informative by coloring the points based on the species of the iris flower.\n\nfig, ax = plt.subplots()\nspecies = df['species'].unique()\n\nfor sp in species:\n    x = df[df['species'] == sp]['sepal_length']\n    y = df[df['species'] == sp]['sepal_width']\n    ax.scatter(x, y, label=sp)\n\nax.set_xlabel('Sepal length')\nax.set_ylabel('Sepal width')\nax.set_title('Sepal length vs Sepal width')\nax.legend()\nplt.show()\n\n\n\n\n\n\n\nFigure 7.6: Scatter plot of sepal length vs sepal width using the iris dataset colored by species.\n\n\n\n\n\nAbove, we loop through the unique species in the dataset and create a scatter plot for each species. We then add a legend to the plot to indicate which species each color corresponds to.\nWe might also want to choose different shapes for the points based on the species of the iris flower. This is not much different from coloring the points based on the species (Figure 7.7).\n\nfig, ax = plt.subplots()\nspecies = df['species'].unique()\nmarkers = ['o', 's', '^']\n\nfor sp, marker in zip(species, markers):\n    x = df[df['species'] == sp]['sepal_length']\n    y = df[df['species'] == sp]['sepal_width']\n    ax.scatter(x, y, label=sp, marker=marker)\n\nax.set_xlabel('Sepal length')\nax.set_ylabel('Sepal width')\nax.set_title('Sepal length vs Sepal width')\nax.legend()\nplt.show()\n\n\n\n\n\n\n\nFigure 7.7: Scatter plot of sepal length vs sepal width using the iris dataset colored by species and different markers.\n\n\n\n\n\nThe difference here is that we pass a list of markers to the scatter() method. The zip() function is used to iterate over two lists simultaneously.\n\n7.2.1 Several subplots\nSubplots can actually be used for displaying multiple plots in the same figure. Let’s create a figure with two subplots: one for the sepal length vs sepal width and another for the petal length vs petal width (Figure 7.8).\n\n# Create a figure with two subplots\nfig, ax = plt.subplots(nrows = 1, ncols = 2, figsize=(6, 3))\n\n# Plot 1\nspecies = df['species'].unique()\nmarkers = ['o', 's', '^']\n\nfor sp, marker in zip(species, markers):\n    x = df[df['species'] == sp]['sepal_length']\n    y = df[df['species'] == sp]['sepal_width']\n    ax[0].scatter(x, y, label=sp, marker=marker)\n\nax[0].set_xlabel('Sepal length')\nax[0].set_ylabel('Sepal width')\nax[0].set_title('Sepal length vs Sepal width')\nax[0].legend()\n\n# tight_layout() adjusts the padding between the subplots to make them fit nicely\nplt.tight_layout()\n\n# Plot 2\nspecies = df['species'].unique()\n\nfor sp, marker in zip(species, markers):\n    x = df[df['species'] == sp]['petal_length']\n    y = df[df['species'] == sp]['petal_width']\n    ax[1].scatter(x, y, label=sp, marker=marker)\n\nax[1].set_xlabel('Petal length')\nax[1].set_ylabel('Petal width')\nax[1].set_title('Petal length vs Petal width')\nax[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 7.8: Side-by-side scatter plots of sepal length vs sepal width and petal length vs petal width using the iris dataset.\n\n\n\n\n\nNow we have a figure with two subplots side by side. Here we used the plt.subplots() function to create a figure with two subplots. The first argument is the number of rows, and the second argument is the number of columns. We then create the two scatter plots and add labels, titles, and legends to each subplot. This works, but we did have to basically write the same lines of codes twice.\nOne way to avoid this is to create a custom function, which takes the x- and y-coordinates as arguments and returns a scatter plot. Let’s see how we can do this (Figure 7.9).\n\ndef plot_scatter(ax, x, y, species, markers):\n    for sp, marker in zip(species, markers):\n        x_sp = x[df['species'] == sp]\n        y_sp = y[df['species'] == sp]\n        ax.scatter(x_sp, y_sp, label=sp, marker=marker)\n\n    ax.set_xlabel(x.name)\n    ax.set_ylabel(y.name)\n    ax.set_title(f'{x.name} vs {y.name}')\n    ax.legend()\n\nfig, ax = plt.subplots(nrows = 1, ncols = 2, figsize=(6, 3))\n\n# Plot 1\nplot_scatter(ax[0], df['sepal_length'], df['sepal_width'], df['species'].unique(), ['o', 's', '^'])\n\n# Plot 2\nplot_scatter(ax[1], df['petal_length'], df['petal_width'], df['species'].unique(), ['o', 's', '^'])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 7.9: Side-by-side scatter plots of sepal length vs sepal width and petal length vs petal width using the iris dataset and a custom plotting function.\n\n\n\n\n\nHere, we were able to achieve a similar result with a more elegant and scalable solution. The custom function we created let’s us create scatter plots with any combination of x- and y-coordinates. This is a good example of how we can use functions to make our code more modular and reusable.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data visualisation with Matplotlib</span>"
    ]
  },
  {
    "objectID": "seaborn.html",
    "href": "seaborn.html",
    "title": "8  Seaborn for data visualisation",
    "section": "",
    "text": "8.1 Getting started\nLet’s start by creating a simple scatter plot using Seaborn. We start by importing Seaborn for the visualizations and Numpy and Pandas for generating and handling the data.\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nNext, we generate some random data and plot it using Seaborn (Figure 8.1).\nx = np.random.normal(size=100)\ny = np.random.normal(size=100)\ndf = pd.DataFrame({'x': x, 'y': y})\n\nsns.scatterplot(x='x', y='y', data=df)\n\n\n\n\n\n\n\nFigure 8.1: Scatter plot created using Seaborn.\nWe can see right away that Seaborn’s syntax differs somewhat from the one used by Matplotlib.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Seaborn for data visualisation</span>"
    ]
  },
  {
    "objectID": "matplotlib.html#different-plot-types",
    "href": "matplotlib.html#different-plot-types",
    "title": "7  Data visualisation with Matplotlib",
    "section": "7.3 Different plot types",
    "text": "7.3 Different plot types\nNow that we have gotten a taste of how Matplotlib works, we can expand to other plot types. Matplotlib supports a wide variety of plot types, including bar plots, histograms, box plots, and violin plots.\n\n\n\n\n\n\nConsulting the documentation\n\n\n\nWhenever you are at a loss on how to create a specific plot, the Matplotlib website and the documentation therein is a good place to start looking for an answer: https://matplotlib.org/\n\n\nWork in progress…",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data visualisation with Matplotlib</span>"
    ]
  },
  {
    "objectID": "seaborn.html#getting-started",
    "href": "seaborn.html#getting-started",
    "title": "8  Seaborn for data visualisation",
    "section": "",
    "text": "Seaborn Documentation\n\n\n\nThe Seaborn website contains something called the API reference, which is a comprehensive guide to the functions and classes in Seaborn. It is a great resource to learn more about the details regarding Seaborn and how to use it effectively. You can find it at https://seaborn.pydata.org/api.html.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Seaborn for data visualisation</span>"
    ]
  },
  {
    "objectID": "seaborn.html#evaluating-distributions",
    "href": "seaborn.html#evaluating-distributions",
    "title": "8  Seaborn for data visualisation",
    "section": "8.2 Evaluating distributions",
    "text": "8.2 Evaluating distributions\nSeaborn offers many convenient plotting functions for evaluating distributions. Let’s explore the penguins dataset that comes with Seaborn. The penguins dataset contains data on the size and species of penguins collected from different islands in the Palmer Archipelago, Antarctica. We can load the dataset using the following code:\n\npenguins = sns.load_dataset('penguins').dropna()\n\npenguins.head()\n\n\n\nTable 8.1: The first few rows of the penguins dataset.\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nMale\n\n\n\n\n\n\n\n\n\n\n\nTable 8.2 shows a summary of the dataset.\n\npenguins.describe()\n\n\n\nTable 8.2: Summary statistics for the penguins dataset.\n\n\n\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\ncount\n333.000000\n333.000000\n333.000000\n333.000000\n\n\nmean\n43.992793\n17.164865\n200.966967\n4207.057057\n\n\nstd\n5.468668\n1.969235\n14.015765\n805.215802\n\n\nmin\n32.100000\n13.100000\n172.000000\n2700.000000\n\n\n25%\n39.500000\n15.600000\n190.000000\n3550.000000\n\n\n50%\n44.500000\n17.300000\n197.000000\n4050.000000\n\n\n75%\n48.600000\n18.700000\n213.000000\n4775.000000\n\n\nmax\n59.600000\n21.500000\n231.000000\n6300.000000\n\n\n\n\n\n\n\n\n\n\n\n\n8.2.1 Pairplot\nThe pairplot() function creates a grid of scatterplots for all pairs of numerical columns in a DataFrame. Additionally, it show’s an estimate of the distribution of each column along the diagonal. It is simple yet effective way to get an overview of your data, provided the data doesn’t entail a huge number of numeric columns. Figure 8.2 shows an example for the penguins dataset.\n\npen_plt = sns.pairplot(penguins, hue='island')\npen_plt.fig.set_size_inches(7,7)\n\n\n\n\n\n\n\nFigure 8.2: Pairplot of the penguins dataset.\n\n\n\n\n\nAbove we used the hue parameter to color the points based on the island where the penguins were observed. This makes it easier to see if there are any differences between the islands. We can set the hue parameter to any categorical column in the dataset.\n\n\n8.2.2 Displot & Histogram\nThe displot() function creates a histogram and a kernel density estimate of the data. It is a convenient way to evaluate the distribution of a single variable. Figure 8.3 shows an example for the penguins dataset.\n\nsns.displot(penguins['flipper_length_mm'], kde=True)\n\n\n\n\n\n\n\nFigure 8.3: Distribution plot of the penguin’s flipper length.\n\n\n\n\n\nIf we are simply looking to create a histogram, we can use the histplot() function. Figure 8.4 shows an example for the penguins dataset.\n\nsns.histplot(penguins['flipper_length_mm'], bins=20)\n\n\n\n\n\n\n\nFigure 8.4: Histogram of the penguin’s flipper length.\n\n\n\n\n\nHere we used the bins parameter to set the number of bins in the histogram. There are many other parameters that can be used to customize the plot, including but not limited to:\n\nbinwidth: Width of each bin\nfill: Whether to fill the bars with color.\n\n\n\n8.2.3 Boxplot\nBoxplot is convenient for visualizing the distribution of a numerical variable across different categories. The boxplot() function creates a boxplot of the data. Figure 8.5 shows an example for the penguins dataset.\n\nsns.boxplot(x='species', y='flipper_length_mm', data=penguins)\n\n\n\n\n\n\n\nFigure 8.5: Boxplot of the penguin’s flipper length. The box shows the positions of the first, second (median), and third quartiles. The whiskers extend to the most extreme data points not considered outliers (according to normal distribution), and the outliers are plotted as individual points.\n\n\n\n\n\n\n\n8.2.4 Jointplot\nThe jointplot() function creates a scatter plot of two numerical variables along with the histograms of each variable. It is a convenient way to visualize the relationship between two variables. Figure 8.6 shows an example with the flipper length and body mass.\n\nsns.jointplot(x='flipper_length_mm', y='body_mass_g', \ndata=penguins, kind='reg')\n\n\n\n\n\n\n\nFigure 8.6: Jointplot showing the correlation between penguin flipper length and body mass.\n\n\n\n\n\nThe kind parameter is very useful, as it allows us to choose the type of plot to display in the jointplot. For example, if we have a lot of points we can use kind='hex' to create a hexbin plot. Figure 8.7 shows an example.\n\nsns.jointplot(x='flipper_length_mm', y='body_mass_g',\ndata=penguins, kind='hex')\n\n\n\n\n\n\n\nFigure 8.7: Jointplot showing the correlation between penguin flipper length and body mass using a hexbin plot. The number of observations within each hexagon is represented by the color intensity.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Seaborn for data visualisation</span>"
    ]
  },
  {
    "objectID": "seaborn.html#comparing-groups",
    "href": "seaborn.html#comparing-groups",
    "title": "8  Seaborn for data visualisation",
    "section": "8.3 Comparing groups",
    "text": "8.3 Comparing groups\nA common task in data analysis is to compare different groups within data. For example, in the penguins dataset we might want to compare the body mass of the different species. One way to do this is to create a barplot, as we have done in Figure 8.8.\n\nsns.barplot(x='species', y='body_mass_g', data=penguins)\n\n\n\n\n\n\n\nFigure 8.8: Barplot showing the average body mass of the different penguin species.\n\n\n\n\n\nBy default, the barplot shows the average value of the y variable for each category in the x variable. The barplot function has a parameter for adjusting which estimator is used in comparing the group. We change the estimator to e.g. median quite easily with the help of numpy as shown in Figure 8.9.\n\nimport matplotlib.pyplot as plt\n\nplot = sns.barplot(x='species', y='body_mass_g', data=penguins, estimator=np.median)\n# re-label y-axis\nplot.set_ylabel('Median body mass (g)')\n\nplt.show()\n\n\n\n\n\n\n\nFigure 8.9: Barplot showing the median body mass of the different penguin species.\n\n\n\n\n\n\n8.3.1 Countplot\nIf the bars in the barplot are used for counting the number of observations, we can use the countplot() function in Seaborn. Figure 8.10 shows an example of this on the number of different species in the penguins dataset. This is a great way of assessing if the dataset is balanced or not.\n\nsns.countplot(x='species', data=penguins)\n\n\n\n\n\n\n\nFigure 8.10: Countplot showing the number of penguin observations per species in the data.\n\n\n\n\n\n\n\n8.3.2 Digging deeper with the hue parameter\nAs we saw above, the countplot and barplot functions are effective tools for comparing groups within a dataset. The hue parameter allows us to add an extra layer of granularity to the plot. For example, we can use the hue parameter to compare the bill lenght of the different species of penguins and see the effect of sex of the penguin. Figure 8.11 shows an example of this.\n\nsns.barplot(x='species', y='bill_length_mm', hue='sex', data=penguins)\n\n\n\n\n\n\n\nFigure 8.11: Barplot showing the average bill length of the different penguin species. The bars are colored according to sex.\n\n\n\n\n\nUsing the hue parameter is very useful also with plot types like boxplot and violinplot, which display distributions. Figure 8.12 shows an example of a boxplot with the hue parameter.\n\nsns.boxplot(x='species', y='flipper_length_mm', hue='sex', data=penguins)\n\n\n\n\n\n\n\nFigure 8.12: Boxplot showing the flipper length of the different penguin species. The boxes are colored according to sex.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Seaborn for data visualisation</span>"
    ]
  },
  {
    "objectID": "seaborn.html#distributions-per-group",
    "href": "seaborn.html#distributions-per-group",
    "title": "8  Seaborn for data visualisation",
    "section": "8.4 Distributions per group",
    "text": "8.4 Distributions per group\nThe hue parameter can also be used to create separate plots for each group. For example, we can use the FacetGrid class to create a separate histogram for each species in the penguins dataset. Figure 8.12 shows an example of this.\n\ng = sns.FacetGrid(penguins, col='species')\ng.map(sns.histplot, 'flipper_length_mm')\n\nC:\\Users\\TeroJalkanen\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\nC:\\Users\\TeroJalkanen\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\nC:\\Users\\TeroJalkanen\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n\n\n\n\n\n\nFigure 8.12: FacetGrid showing the distribution of flipper length for each penguin species.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Seaborn for data visualisation</span>"
    ]
  },
  {
    "objectID": "seaborn.html#faceting",
    "href": "seaborn.html#faceting",
    "title": "8  Seaborn for data visualisation",
    "section": "8.4 Faceting",
    "text": "8.4 Faceting\nIn the previous section we saw how to utilize the hue parameter in separating data based on a grouping variable. Another useful technique for visualizing grouped data is faceting. We can use the FacetGrid class to create a separate histogram for each species in the penguins dataset. Figure 8.13 shows an example of this.\n\ng = sns.FacetGrid(penguins, col='species')\ng.map(sns.histplot, 'flipper_length_mm')\ng.fig.set_size_inches(w=7, h=3)\n\n\n\n\n\n\n\nFigure 8.13: FacetGrid showing the distribution of flipper length for each penguin species. By setting the species column as the col parameter value, separate plots for each species are created into their respective columns.\n\n\n\n\n\nWe can also use the row parameter to facet the plots into rows. This is especially useful when we have two categorical columns we want to group by. Figure 8.14 shows data grouped by the species and sex variables.\n\n# modify the sex column to show F for Female and M for Male\npenguins['sex'] = penguins['sex'].apply(lambda x: 'F' if x == 'Female' else 'M')\n\ng = sns.FacetGrid(penguins, row='sex', col='species')\ng.map(sns.scatterplot, 'bill_length_mm', 'bill_depth_mm')\ng.figure.set_size_inches(w=7, h=6)\n\n\n\n\n\n\n\nFigure 8.14: FacetGrid showing the correlation of bill length and bill depth for each penguin species. The plots are faceted into rows based on sex.\n\n\n\n\n\n\n8.4.1 More complex Grids\nFacetGrid is not the only way to create a grid of plots in Seaborn. The pairplot function we saw earlier is another example of a grid of plots. In fact pairplot is just a specialized version of a more general function called PairGrid. We can use PairGrid to create a grid of plots for any pair of variables in the dataset, and we can also specify which type of plots we want on different parts of the grid. Figure 8.15 shows an example of this.\n\ng = sns.PairGrid(penguins, hue='species')\n# set the upper triangle to scatterplot\ng.map_upper(sns.scatterplot)\n# set the lower triangle to kdeplot\ng.map_lower(sns.kdeplot)\n# set the diagonal to histplot\ng.map_diag(sns.histplot)\ng.add_legend()\nsns.move_legend(g, \"lower center\",\n    bbox_to_anchor=(.5, 1), ncol=3, title=None, frameon=False\n    )\n\n# set figure size\ng.figure.set_size_inches(w=6, h=6)\n\n\n\n\n\n\n\nFigure 8.15: PairGrid showing the pairwise relations of all numeric variables in the penguins dataset.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Seaborn for data visualisation</span>"
    ]
  },
  {
    "objectID": "seaborn.html#visualizing-correlation",
    "href": "seaborn.html#visualizing-correlation",
    "title": "8  Seaborn for data visualisation",
    "section": "8.5 Visualizing Correlation",
    "text": "8.5 Visualizing Correlation\nCorrelation can be roughly described as a measure of the strength and direction of a linear relationship between two numerical variables. For example, for the numeric columns in the penguins dataset, we can calculate the correlation matrix using the corr() function. Table 8.3 shows the correlation matrix for the penguins dataset.\n\npen_corr = penguins.corr(numeric_only=True)\npen_corr\n\n\n\nTable 8.3: A matrix showing the Pearson correlation between the numerical columns in the penguins dataset.\n\n\n\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\nbill_length_mm\n1.000000\n-0.228626\n0.653096\n0.589451\n\n\nbill_depth_mm\n-0.228626\n1.000000\n-0.577792\n-0.472016\n\n\nflipper_length_mm\n0.653096\n-0.577792\n1.000000\n0.872979\n\n\nbody_mass_g\n0.589451\n-0.472016\n0.872979\n1.000000\n\n\n\n\n\n\n\n\n\n\n\nOnce we have our data in this rectangular form, we can use the heatmap() function, which offers a convenient way to visualize the correlation. The heatmap() function creates a color-coded matrix that shows the correlation between each pair of variables. Figure 8.16 shows an example of this for the correlation data we created above.\n\nsns.heatmap(pen_corr, annot=True)\n\n\n\n\n\n\n\nFigure 8.16: Heatmap showing the correlation between numerical variables in the penguins dataset. Color intensity is used to visualize the correlation values.\n\n\n\n\n\nThe annot parameter is used to display the numeric correlation values in the heatmap. Moreover, cmap parameter can be used to change the color map of the heatmap.\n\n8.5.1 Heatmap for temporal data\nHeatmaps can also be used to visualize temporal data. Table 8.4 shows monthly weather data from the website Our World in Data.\n\nweather = pd.read_csv('data/monthly-average-surface-temperatures-by-year.csv')\nweather\n\n\n\nTable 8.4: The finnish weather dataset from Our World in Data.\n\n\n\n\n\n\n\n\n\n\n\nEntity\nCode\nYear\n2024\n2023\n2022\n2021\n2020\n2019\n2018\n...\n1959\n1958\n1956\n1954\n1952\n1957\n1955\n1953\n1951\n1950\n\n\n\n\n0\nFinland\nFIN\n1\n-12.356432\n-4.536643\n-7.592226\n-9.110156\n-3.112542\n-11.025353\n-6.895465\n...\n-9.696966\n-12.954665\n-11.878848\n-8.946227\n-5.870114\n-6.206512\n-9.807618\n-9.158957\n-10.899360\n-14.994019\n\n\n1\nFinland\nFIN\n2\n-7.631946\n-5.592857\n-6.273704\n-11.295887\n-4.104153\n-5.959055\n-11.653291\n...\n-4.385164\n-11.575406\n-16.195097\n-13.229277\n-5.858663\n-6.452776\n-13.163642\n-13.099696\n-8.866541\n-8.616582\n\n\n2\nFinland\nFIN\n3\n-2.152660\n-6.260747\n-2.362002\n-3.575844\n-2.093242\n-3.754119\n-8.216735\n...\n-1.374446\n-9.041509\n-6.101201\n-3.394979\n-9.736150\n-8.902771\n-8.678738\n-3.652206\n-7.219227\n-4.774562\n\n\n3\nFinland\nFIN\n4\n-0.105814\n1.859364\n0.963580\n1.959561\n0.592504\n3.668244\n1.857609\n...\n0.429239\n-1.223686\n-2.926554\n-0.158412\n1.350001\n-0.055782\n-3.018537\n2.357863\n1.580248\n2.926893\n\n\n4\nFinland\nFIN\n5\n10.342938\n9.054110\n7.681894\n7.209668\n6.447229\n7.416959\n11.818787\n...\n7.576959\n5.305153\n7.060782\n9.387682\n5.249408\n6.424380\n3.803955\n7.328690\n4.187241\n6.869668\n\n\n5\nFinland\nFIN\n6\n15.629230\n14.390386\n14.857914\n16.784351\n16.298048\n14.332166\n12.409929\n...\n13.862224\n12.240686\n14.262403\n12.196805\n12.959100\n10.485080\n10.143443\n16.841997\n11.469393\n13.351920\n\n\n6\nFinland\nFIN\n7\n17.379380\n15.121183\n16.573692\n18.357800\n14.814271\n14.564145\n19.720356\n...\n16.012850\n13.836065\n14.284949\n16.679026\n14.955886\n17.375101\n16.000717\n15.119339\n13.776219\n14.620484\n\n\n7\nFinland\nFIN\n8\n15.999804\n15.713537\n15.900022\n13.261284\n14.121853\n13.773454\n15.358797\n...\n14.763309\n13.833643\n11.212431\n13.601360\n11.714494\n13.959742\n15.751595\n13.766891\n16.507440\n15.331268\n\n\n8\nFinland\nFIN\n9\nNaN\n12.078142\n7.694565\n7.075720\n9.989091\n8.631719\n10.090892\n...\n6.644055\n8.106494\n6.433380\n9.015210\n6.523679\n7.595360\n10.376423\n6.875361\n9.839784\n9.541843\n\n\n9\nFinland\nFIN\n10\nNaN\n0.258406\n4.493286\n4.510300\n4.983604\n1.240841\n3.102760\n...\n2.086031\n3.264047\n0.802558\n1.842529\n-0.712148\n2.896886\n1.894943\n4.152942\n5.239940\n4.349140\n\n\n10\nFinland\nFIN\n11\nNaN\n-5.321711\n-1.655154\n-2.792642\n1.341732\n-3.120430\n0.699558\n...\n-2.421227\n0.332971\n-8.738729\n-3.450766\n-4.315333\n-2.058513\n-6.467130\n-0.935875\n-4.115920\n-2.016795\n\n\n11\nFinland\nFIN\n12\nNaN\n-8.636971\n-6.510187\n-9.504569\n-2.615182\n-2.354235\n-5.373116\n...\n-9.508020\n-11.801905\n-6.794479\n-2.086087\n-7.242684\n-6.743253\n-16.398590\n-2.729617\n-4.081283\n-5.246907\n\n\n\n\n12 rows × 78 columns\n\n\n\n\n\n\n\nWe can see that the dataset contains the average temperature for each month in Finland from 1950 to 2024. We can use the heatmap() function to visualize this data. When we look at the data in Table 8.4 we notice that the months are actually listed under the Year column, while the years themselves are listed with numeric column names. First thing to do, is to move the Year column as the index and remove the Entity and Code columns. Finally, we shall transpose the data, so that the months will be displayed on the x-axis of the resulting heatmap (Figure 8.17).\n\nweather = weather.set_index('Year').drop(columns=['Entity', 'Code'])\n# rename index to month\nweather.index.name = 'Month'\n# transpose the data\nweather = weather.T\nsns.heatmap(weather, cmap='coolwarm')\n\n\n\n\n\n\n\nFigure 8.17: Heatmap showing the average temperature in Finland for each month from January of 1950 until August of 2024. Color intensity is used to visualize the temperature values.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Seaborn for data visualisation</span>"
    ]
  },
  {
    "objectID": "seaborn.html#visualizing-regression",
    "href": "seaborn.html#visualizing-regression",
    "title": "8  Seaborn for data visualisation",
    "section": "8.6 Visualizing Regression",
    "text": "8.6 Visualizing Regression\nSeaborn is designed for statistical data visualization. With this in mind, it is not that surprizing to learn that Seaborn offers many convenient ways to visualize regression models. The lmplot() and the regplot() functions are the two function offered by Seaborn to visualize a linear fit. You can refer to the Seaborn documentation about the differences between the two functions, but briefly the lmplot() function requires the data argument to be passed, whereas regplot() can be used for plotting e.g. two numpy arrays. Figure 8.18 shows an example of the lmplot() function with the penguins dataset, and Figure 8.19 shows an example of the regplot() function with two numpy arrays.\n\nsns.lmplot(x='flipper_length_mm', y='body_mass_g', data=penguins)\n\n\n\n\n\n\n\nFigure 8.18: The linear relationship between flipper length and body mass of penguins.\n\n\n\n\n\n\n# regplot example for numpy arrays\nsns.regplot(x=np.random.normal(size=10), y=np.random.normal(size=10))\n\n\n\n\n\n\n\nFigure 8.19: The relationship between two random numpy arrays, with a line fitted through the data.\n\n\n\n\n\nSo, as we can see, both functions offer a way to visualize the linear relationship between two variables.\nWe can utilize similar techniques we saw earlier with the lmplot() and regplot() functions. For example, we can use the hue parameter to color the points based on the species of the penguin, and facet according to sex (Figure 8.20).\n\ng = sns.lmplot(data=penguins, x='flipper_length_mm', y='body_mass_g', \n  hue='species', col='sex', \n  aspect=1, height=3.5\n  )\nsns.move_legend(g, \"lower center\",\n    bbox_to_anchor=(.5, 1), ncol=3, title=None, frameon=False\n    )\n\n\n\n\n\n\n\nFigure 8.20: The linear relationship between flipper length and body mass of penguins separated by sex, where a individual linear fits have been made for each species.\n\n\n\n\n\n\n8.6.1 Beyond the linear fit\nThe lmplot() function is not limited to fitting straight lines. In fact, we can use the order parameter to fit a polynomial regression model. Figure 8.21 demonstrates fitting a second order polynomial to the penguins data.\n\nsns.lmplot(x='flipper_length_mm', y='body_mass_g', data=penguins, order=2)\n\n\n\n\n\n\n\nFigure 8.21: A second degree polynomial has been fitted to the flipper length and body mass data.\n\n\n\n\n\nLogistic regression models are used in binary classification tasks. The logistic curve is characterized by a shape resembling the letter S. We will learn more about logistic regression in the upcoming section. For now, let’s fit a logistic regression model to the penguins data, and see how we can create a visualization to help distinguish between Male and Female Gentoo penguins. We can use the logistic parameter to fit a logistic model after we recode the Male and Female classes to 1 and 0, respectively (Figure 8.22).\n\n# recoding the sex column to be 1 for males and 0 for females\npenguins['sex'] = penguins['sex'].apply(lambda x: 1 if x == 'M' else 0)\nlog_data = penguins[penguins['species'] == 'Gentoo']\n\nsns.lmplot(data = log_data, x='body_mass_g', y='sex', logistic=True,\n  height=4, aspect=1.5)\n\n\n\n\n\n\n\nFigure 8.22: A logistic regression fit depicting sex and a function of body mass for Gentoo penguins.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Seaborn for data visualisation</span>"
    ]
  },
  {
    "objectID": "seaborn.html#conclusion",
    "href": "seaborn.html#conclusion",
    "title": "8  Seaborn for data visualisation",
    "section": "8.7 Conclusion",
    "text": "8.7 Conclusion\nThere are many more functions and features in Seaborn that we are yet to cover. Luckily, we will be using Seaborn in the upcoming section to create visualizations useful for different modelling tasks. This will give us a chance to explore more of Seaborn’s capabilities. Furthermore, you can always refer to the Seaborn documentation for more information.",
    "crumbs": [
      "Data Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Seaborn for data visualisation</span>"
    ]
  },
  {
    "objectID": "functions.html#conclusion",
    "href": "functions.html#conclusion",
    "title": "3  Functions",
    "section": "3.6 Conclusion",
    "text": "3.6 Conclusion\nThe first three chapters introduced us to the basics of Python programming. Next we will start discussing how to use the language for processing and analyzing data. In case you are looking to strengthen your understanding of the fundamentals, the Python tutorial on the official Python website is a great place to learn more (Python Software Foundation (2024)).\n\n\n\n\nPython Software Foundation. 2024. “The Python Tutorial.” https://docs.python.org/3/tutorial/.",
    "crumbs": [
      "The Very Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "linear_regression.html#simple-linear-regression",
    "href": "linear_regression.html#simple-linear-regression",
    "title": "9  Linear regression",
    "section": "",
    "text": "pip install scikit-learn\n\n\n\n\n\n9.1.1 Fitting the model with scikit-learn\nAbove we used our seaborn knowledge to visualize the linear relationship. However, this doesn’t actually give as a model with stored parameter values. In order to actually fit the model shown in Equation 9.1, we can use LinearRegression from scikit-learn.\n\nfrom sklearn.linear_model import LinearRegression\n\n# create a linear regression model object\nmodel = LinearRegression()\n\n# fit the model to the data\nmodel.fit(X=happiness[['GDP per capita']], y=happiness['Score'])\n\n# print the model coefficients\nmodel.coef_, model.intercept_\n\n(array([2.218148]), 3.399345178292417)\n\n\nLet’s go throught the code above line by line. First, we import the LinearRegression Estimator from the sklearn.linear_model family of models. Next, we create a LinearRegression object called model. We then fit the model to the data using the fit method. The fit method takes two arguments: the independent variable X and the dependent variable y. In this case, we are using GDP per capita as the independent variable and Score as the dependent variable. Finally, we print the model coefficients. The coef_ attribute contains the slope of the line, and the intercept_ attribute contains the intercept.\nLet’s now look at Figure 9.1 again, to help us understand the model we just fitted. The slope of the line is the coefficient for GDP per capita (\\(\\beta_1\\) in Equation 9.1), and the intercept is the value of the dependent variable when GDP per capita is zero. In other words, the intercept is the y-value where the fitted line crosses the y-axis. By looking at the plot, we can see that the intercept is around 3.4, which is the result we got from our fitted model as well. The slope is around 2.2, which means that for every unit increase in GDP per capita, the Score increases by 2.2 units. This is the beauty of linear regression - it gives us a simple and interpretable model that we can use to make predictions.\n\n\n9.1.2 Making predictions\nNow that we have a fit for our model to the data, we can use it to make predictions. In this case the model is very simple, so we could easily calculate the predictions by hand as well. Let’s predict the happiness score for the countries in the dataset. The results for the first five entries are displayed in Table 9.2.\n\n# make a prediction for all the countries in the dataset\nX_new = happiness[['GDP per capita']]\npreds = model.predict(X_new)\n\n## create a dataframe with the predictions, GDP per capita and the actual score\npreds_df = pd.DataFrame({\n    'GDP per capita': X_new['GDP per capita'],\n    'Predicted score': preds,\n    'Actual score': happiness['Score']\n})\n\npreds_df.head()\n\n\n\nTable 9.2: The actual happiness score and the predicted score based on the GDP per capita.\n\n\n\n\n\n\n\n\n\n\n\nGDP per capita\nPredicted score\nActual score\n\n\n\n\n0\n1.340\n6.371663\n7.769\n\n\n1\n1.383\n6.467044\n7.600\n\n\n2\n1.488\n6.699949\n7.554\n\n\n3\n1.380\n6.460389\n7.494\n\n\n4\n1.396\n6.495880\n7.488\n\n\n\n\n\n\n\n\n\n\n\nThe GDP per capita value for the first entry is 1.34. Let’s calculate the predicted score “by hand” using the model coefficients:\n\n# calculate the predicted score for the first entry\npredicted_score = model.intercept_ + model.coef_[0] * 1.34\npredicted_score\n\n6.371663499643615\n\n\nLo and behold, we get the same result as by using the predict method (as we should).\n\n\n9.1.3 Evaluating model performance\nNow that we have our predictions, we are ready to evaluate how successful we were with the fitting process. There are different ways to evaluate the performance of a regression model, but we are going to look at three commonly used metrics. Namely:\n\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\nMean Absolute Error (MAE).\n\nMSE in defined as follows:\n\\[\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\text{,}\n\\]\nwhere \\(y_i\\) is the actual value for the \\(i\\)th observation, \\(\\hat{y}_i\\) is the corresponding predicted value, and \\(n\\) is the number of observations. So basically we are just looking at the average of the squared differences between the actual and predicted values. RMSE is simply the square root of the MSE:\n\\[\n\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} \\text{.}\n\\]\nMAE is possibly the most intuitive of the three, as it is simply the average of the absolute differences between the actual and predicted values:\n\\[\n\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| \\text{.}\n\\]\nWe can now calculate these metrics for our model. Let’s add squared and absolute differences to the predictions dataframe and calculate the metrics. Table 9.3 shows the head of the updated dataframe.\n\n# calculate the squared and absolute differences\npreds_df['Sqr. diff'] = (preds_df['Actual score'] - preds_df['Predicted score'])**2\npreds_df['Abs. diff'] = abs(preds_df['Actual score'] - preds_df['Predicted score'])\n\npreds_df.head()\n\n\n\nTable 9.3: The squared and absolute differences between the actual and predicted scores for the first five entries.\n\n\n\n\n\n\n\n\n\n\n\nGDP per capita\nPredicted score\nActual score\nSqr. diff\nAbs. diff\n\n\n\n\n0\n1.340\n6.371663\n7.769\n1.952549\n1.397337\n\n\n1\n1.383\n6.467044\n7.600\n1.283590\n1.132956\n\n\n2\n1.488\n6.699949\n7.554\n0.729402\n0.854051\n\n\n3\n1.380\n6.460389\n7.494\n1.068351\n1.033611\n\n\n4\n1.396\n6.495880\n7.488\n0.984303\n0.992120\n\n\n\n\n\n\n\n\n\n\n\nWe can now calculate the MSE, RMSE, and MAE for the model.\n\nMSE = preds_df['Sqr. diff'].mean()\nRMSE = MSE**0.5\nMAE = preds_df['Abs. diff'].mean()\n\nMSE, RMSE, MAE\n\n(0.4551967397324354, 0.6746826955928508, 0.5490499282630026)\n\n\nScikit-learn also provides a convenient way to calculate these metrics using the mean_squared_error and mean_absolute_error functions from the sklearn.metrics module. Let’s see if we get the same results using these functions.\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nMSE = mean_squared_error(happiness['Score'], preds)\nRMSE = mean_squared_error(happiness['Score'], preds, squared=False)\nMAE = mean_absolute_error(happiness['Score'], preds)\n\nMSE, RMSE, MAE\n\n(0.4551967397324354, 0.6746826955928508, 0.5490499282630026)\n\n\nPhew, what a relief! The results are the same as with our manual calculation. These convenient functions are actually the one’s we’ll be using from now on.\nWe have now successfully fitted a simple linear regression model to the data, made predictions, and evaluated the model’s performance. Unfortunately, as it turns out, we have taken some shortcuts in the process which we need to address.\n\n\n\n\n\n\nWarning\n\n\n\nYou should never use all you data to fit a model. The reason is that the model will be biased towards the data it has seen, and has a higher probability of not generalizing well to new data. There is a large risk of something called overfitting, which we will learn more about in the next section.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "environment.html",
    "href": "environment.html",
    "title": "Appendix B — Setting up a Conda environment",
    "section": "",
    "text": "How to setup a Conda environment.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Setting up a Conda environment</span>"
    ]
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Appendix A — Installing Python",
    "section": "",
    "text": "How to install Python.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Installing Python</span>"
    ]
  },
  {
    "objectID": "linear_regression.html#re-fitting-the-model-the-proper-way",
    "href": "linear_regression.html#re-fitting-the-model-the-proper-way",
    "title": "9  Linear regression",
    "section": "9.2 Re-fitting the model the proper way",
    "text": "9.2 Re-fitting the model the proper way\nIn the previous example, we used only one feature to predict the happiness score. However, oftentimes we want to use multiple features to make the prediction. This is called multiple linear regression. The equation for multiple linear regression is:\n\\[\ny = \\beta_0 + \\sum_{i=1}^{n} \\beta_i x_i \\text{,}\n\\tag{9.2}\\]\nwhere \\(y\\) is the dependent variable, \\(x_1, x_2, \\ldots, x_n\\) are the independent variables, \\(\\beta_0\\) is the intercept, and \\(\\beta_1, \\beta_2, \\ldots, \\beta_n\\) are the coefficients for each independent variable.\nNow, there is actually nothing wrong with using only a single feature to predict the happiness score. However, as we discussed above, you shouldn’t use all your data to fit the model. Instead, you should split the data into training and test sets, and fit the model using only the training data. We’ll learn more about this next.\n\n9.2.1 The train-test split\nSplitting the data into training and test sets is a crucial step in the machine learning workflow. It is such a standard procedure that scikit-learn comes with a built in function to help us get the job done. This is important because we want to evaluate the model on data that it hasn’t seen before. This helps prevent overfitting, and let’s us see how well the model performs on new data. However, before we are ready to split the data into training and test sets, we first need to decide which features to use in our model. This usually means dividing the data into features (X) and the target variable (y). We’ll use all the numeric columns except the Score column as features, and the Score column as the target variable.\n\nX = happiness[['GDP per capita', 'Social support', 'Healthy life expectancy', \n'Freedom to make life choices', 'Generosity', \n'Perceptions of corruption']]\ny = happiness['Score']\n\nNow that our data is setup correctly, we can split it into training and test sets. To do this, we use the train_test_split function from scikit-learn.\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=123\n    )\n\nAbove we passed the X and y dataframes to the train_test_split function. The test_size parameter specifies the proportion of the data that should be used for the test set. In this case, we used 20% of the data for the test set. The random_state parameter is used to ensure that the split is reproducible. By setting the random state to a fixed value, we get the same “random” split every time we run the code.\nBefore fitting the model, let’s take a look at the shape of the training and test sets.\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n((124, 6), (32, 6), (124,), (32,))\n\n\nWe can see that the training set contains 124 observations and the test set contains 32 observations. So roughly 80% of the data is used for training and 20% for testing, just as we requested.\nWe are now ready to fit the model using the training data.\n\nmodel2 = LinearRegression()\n\n# fit the model to the training data\nmodel2.fit(X_train, y_train)\n\n# save the model coefficients and intercept to a dataframe\ncoefficients = pd.DataFrame({\n    'feature': ['Intercept'] + X.columns.tolist(),\n    # intercept and coefficients\n    'value': [model2.intercept_] + model2.coef_.tolist()\n})\n\nHere we only used the training data to fit the model. This means that the model has never seen the test data before, and we can use it for getting an unbiased estimate of the model’s performance. Table 9.4 shows the coefficients for each feature in the fitted linear regression model.\n\n\n\n\nTable 9.4: The intercept and coefficient values for each feature in the fitted linear regression model.\n\n\n\n\n\n\n\n\n\n\n\nfeature\nvalue\n\n\n\n\n0\nIntercept\n1.668718\n\n\n1\nGDP per capita\n0.864532\n\n\n2\nSocial support\n1.038075\n\n\n3\nHealthy life expectancy\n1.113857\n\n\n4\nFreedom to make life choices\n1.651626\n\n\n5\nGenerosity\n0.982648\n\n\n6\nPerceptions of corruption\n0.853486\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.2.2 Evaluating the model\nLet’s now evaluate our second model using the test data and the metrics we discussed earlier. In order to do this, we need to first make predictions with our test data.\n\n# make predictions on the test data\ny_pred = model2.predict(X_test)\n\n# calculate the MSE, RMSE, and MAE\nMSE = mean_squared_error(y_test, y_pred)\nRMSE = mean_squared_error(y_test, y_pred, squared=False)\nMAE = mean_absolute_error(y_test, y_pred)\n\nMSE, RMSE, MAE\n\n(0.3104436244813814, 0.5571746804022787, 0.43781582428696403)\n\n\nThese metrics give us an idea of how well the model is performing. The lower the values, the better the model. Generally, there is a distinction between the training and test error. The numbers above show the test error, which is the most important one as it gives us an indication of how well the model performs on unseen data. Unfortunately, the numerical metrics shown above aren’t always that easy to interpret, which is why visualizations can be helpful. Figure 9.2 shows the actual and predicted happiness scores for the test data. In this case we see that our predictions are fairly well in line with the actual scores.\n\nimport matplotlib.pyplot as plt\n\nplot = sns.scatterplot(x=y_test, y=y_pred)\n# add axis labels\nplot.set_xlabel('Actual score')\nplot.set_ylabel('Predicted score')\n# add a diagonal line\nplt.plot([3, 8], [3, 8], color='red', linewidth=1)\n\nplt.show()\n\n\n\n\n\n\n\nFigure 9.2: A scatter plot showing the actual and predicted happiness scores for the test data. For a perfect prediction all the points would be lined up in a straight line (with a slope of 1).\n\n\n\n\n\nThe linear regression is a fairly rigid method in terms of how well it can accommodate to the training data, which means that the risk of overfitting is not as high as with more complex and flexible models. However, it is important to learn to use the train-test split every time you fit a model, regardless of the model complexity.\n\n\n\n\n\n\nBias-variance trade-off\n\n\n\nWhen discussing model performance, it is important to understand the concept of the bias-variance trade-off. Bias refers to the error that is introduced by approximating a possibly complex real-world problem, with a simple model such as linear regression. Variance, on the other hand, refers to the error that is introduced by using a model that can accommodate the random noise in the data. If we are using a very flexible model, such as a deep neural network, our model coefficients might change a lot depending on the training data. Ideally we would not want this to happen, as it might mean that our model is not generalizing well to new data. The idea of the test-train split is to help us avoid overfitting our model to the training data. For a more in-depth discussion on the bias-variance trade-off, I recommend the book by James et al. (James et al. (2023)).\n\n\nNow we have the metrics, and we have the estimates for our model coefficients. What if we want to know how confident we can be with these estimates? What if we want to know which features are the most important for predicting the happiness score? For this we need to have an idea of the statistical significance of the coefficients.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "linear_regression.html#re-fitting-our-model-with-statsmodels",
    "href": "linear_regression.html#re-fitting-our-model-with-statsmodels",
    "title": "9  Linear regression",
    "section": "9.3 Re-fitting our model with statsmodels",
    "text": "9.3 Re-fitting our model with statsmodels\nscikit-learn is a great library for building machine learning models, but in case of linear regression it doesn’t provide a way to readily evaluate the statistical significance of the coefficients. For this we can use a Python module called statsmodels. You should already be familiar how to install new libraries by now, so let’s see how we can use statsmodels to fit a linear regression model.\n\nimport statsmodels.api as sm\n\n# add a constant to the features\nX_train = sm.add_constant(X_train)\n\n# create a linear regression model\nmodel3 = sm.OLS(y_train, X_train)\nresults = model3.fit()\n\n# print the summary of the model\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Score   R-squared:                       0.784\nModel:                            OLS   Adj. R-squared:                  0.773\nMethod:                 Least Squares   F-statistic:                     70.73\nDate:                Sat, 23 Nov 2024   Prob (F-statistic):           1.33e-36\nTime:                        19:08:33   Log-Likelihood:                -94.331\nNo. Observations:                 124   AIC:                             202.7\nDf Residuals:                     117   BIC:                             222.4\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n================================================================================================\n                                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------------------\nconst                            1.6687      0.238      7.000      0.000       1.197       2.141\nGDP per capita                   0.8645      0.253      3.412      0.001       0.363       1.366\nSocial support                   1.0381      0.267      3.882      0.000       0.509       1.568\nHealthy life expectancy          1.1139      0.368      3.028      0.003       0.385       1.842\nFreedom to make life choices     1.6516      0.419      3.938      0.000       0.821       2.482\nGenerosity                       0.9826      0.569      1.725      0.087      -0.145       2.110\nPerceptions of corruption        0.8535      0.594      1.438      0.153      -0.322       2.029\n==============================================================================\nOmnibus:                        8.281   Durbin-Watson:                   1.968\nProb(Omnibus):                  0.016   Jarque-Bera (JB):                8.287\nSkew:                          -0.513   Prob(JB):                       0.0159\nKurtosis:                       3.742   Cond. No.                         28.0\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nWe used the same X_train data as before, but this time we added a constant to the features (this is something that statsmodels needs for fitting the intercept). We then created an OLS (Ordinary Least Squares) model object and fit it to the training data. Finally, we printed the summary of the model.\nAs a printout we get a lot of stuff. Most importantly, however, we can see that the coefficient values are the same as with scikit-learn. Moreover, we now have 95% confidence intervals for the coefficients, as well as p-values for testing the null hypothesis that the coefficient is equal to zero.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "logistic_regression.html",
    "href": "logistic_regression.html",
    "title": "10  Logistic regression",
    "section": "",
    "text": "10.1 Univariate logistic regression\nIn univariate logistic regression, we have a single independent variable and a binary dependent variable. The goal is to estimate the probability that the dependent variable is 1, based on the value of the independent variable. This implies that the values of the categorical dependent variable are assigned so-called dummy values, and are therefore coded as 0 and 1. For example, if we are trying to predict whether a student will pass or fail an exam based on the number of hours they studied, we can code the dependent variable as 1 if the student passed and 0 if they failed.\nThe logistic function is used to model the relationship between the inpedendent and dependent variables. The logistic function is defined as:\n\\[\ny = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}},\n\\tag{10.1}\\]\nwhere \\(y\\) is again the dependent variable, and \\(x\\) is the independent variable, and \\(\\beta_0\\) and \\(\\beta_1\\) are the parameters of the model. As you might have noticed, the exponential term contains the linear regression equation. This is not a coincidence. The logistic regression uses something called the sigmoid function, which basically transforms the output of the linear regression into a probability. The sigmoid function is defined as:\n\\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}.\n\\tag{10.2}\\]\nIf we plot out the sigmoid function, we can see that it has a distinct shape, which somewhat resembles the letter S. Figure 10.1 shows the sigmoid function with \\(z\\) on the x-axis and \\(\\sigma(z)\\) on the y-axis.\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\nz = np.linspace(-10, 10, 100)\n\nplt.plot(z, sigmoid(z))\nplt.xlabel('z')\nplt.ylabel('σ(z)')\nplt.title('Sigmoid function')\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 10.1: The shape of the sigmoid function has a distinct S-like shape.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  },
  {
    "objectID": "linear_regression.html#conclusion",
    "href": "linear_regression.html#conclusion",
    "title": "9  Linear regression",
    "section": "9.4 Conclusion",
    "text": "9.4 Conclusion\nIn this chapter we learned about linear regression, which is a simple yet powerful method for regression analysis. In the next chapter we will get our feet wet with classification, which is also a type of supervised learning problem where the goal is to predict the class of an observation based on the values of the independent variables. We will be using logistic regression, which is a classification algorithm that is closely related to linear regression.\n\n\n\n\n\n\nSupervised learning\n\n\n\nSupervised learning is a concept in machine learning where our data contains the correct answers. This means that we have a dataset where we know the values of the dependent variable for each observation. The goal of supervised learning is to connect these correct answers to the predictors, so that we can make predictions on new data. Regression and classification are two types of supervised learning problems. There is also unsupervised learning, which we will learn more about later.\n\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2023. An Introduction to Statistical Learning with Applications in Python. Springer. https://www.statlearning.com/.\n\n\nKaggle Datasets. 2024. “World Happiness Report - Dataset.” https://www.kaggle.com/datasets/unsdsn/world-happiness?select=2019.csv.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "logistic_regression.html#univariate-logistic-regression",
    "href": "logistic_regression.html#univariate-logistic-regression",
    "title": "10  Logistic regression",
    "section": "",
    "text": "10.1.1 Evaluating the model\nSomething about the confusion matrix.",
    "crumbs": [
      "Modelling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Logistic regression</span>"
    ]
  }
]