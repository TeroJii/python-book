---
jupyter: python3
---

# Working with DataFrames

Now that we have learned how to create DataFrames, and some basic operations, let's deepen our understanding with a few more tricks that help us work with DataFrames more efficiently. We'll start of by importing the necessary libraries.

```{python}
#| code-fold: false
import numpy as np
import pandas as pd
```

Additionally, we will use a sample dataset to demonstrate the operations. Let's load the dataset from the seaborn library.

:::{.callout-note}
Seaborn is a commonly used library for data visualization in Python. It comes with a few sample datasets that we can use for practice. We'll use the classic `iris` dataset for this tutorial. We'll learn more about Seaborn in the upcoming chapters.
:::

```{python}
#| code-fold: false
import seaborn as sns

# Load the dataset
df = sns.load_dataset('iris')
df.head()
```

As we can see, the dataset contains four features: `sepal_length`, `sepal_width`, `petal_length`, and `petal_width`. The target variable is `species`, which is the type of iris flower. We also used the `head()` method to display the first five rows of the dataset. Let's learn more about the dataset using some common methods found in Pandas.

## Common methods in Pandas

We already saw that the `head()` method displays the first five rows of the dataset. Five is the default value, but we can change in using the `n` parameter. Similarly, we can use the `tail()` method to display the last rows of the dataset.

```{python}
#| code-fold: false

# Display the last nine rows
df.tail(9)
```

### General information about the dataset

The `info()` method provides a concise summary of the dataset. It displays the number of non-null values in each column, the data type of each column, and the memory usage of the dataset.

```{python}
#| code-fold: false

df.info()
```

Summary statistics can be obtained using the `describe()` method. It provides the count, mean, standard deviation, minimum, maximum, and the quartiles of the dataset. This provides a handy way of getting an overall impression of the dataset.

```{python}
#| code-fold: false

df.describe()
```

If we just want to know the number of rows and columns in the dataset, we can use the `shape` attribute.

```{python}
#| code-fold: false

df.shape
```

The `columns` attribute provides the names of the columns in the dataset.

```{python}
#| code-fold: false

df.columns
```

### Unique values in the dataset

There are a few methods that help us get unique values in the dataset. The `unique()` method provides the unique values in a column. Let's see how this works by getting the unique values in the `species` column.

```{python}
#| code-fold: false

unique_species = df['species'].unique()
unique_species
```

We can see that the dataset contains three unique species of iris flowers. This is pretty simple to see directly, but if we would have more than three species, we could also check the number of unique values like this:

```{python}
#| code-fold: false

len(unique_species)
```

There is actually a dedicated method for obtaining the number of unique values. The `nunique()` method provides the number of unique values in each column of the dataset.

```{python}
#| code-fold: false

df.nunique()
```

## Grouping data

If you are familiar with SQL queries, you might have used the `GROUP BY` clause to group data based on a particular column. Pandas provides a similar functionality using the `groupby()` method. Grouping data allows you to calculate for example statistics like the mean for distinct groups in the dataset. Let's take a look at how this works by grouping the `iris` data based on the `species` column and calculating the mean of the other columns.

```{python}
#| code-fold: false

df.groupby('species').mean()
```

We can also target specific columns. Let's see how this works by taking the sum of the `sepal_length` for each species.

```{python}
#| code-fold: false

df.groupby('species')['sepal_length'].sum()
```

Some commonly used aggregation functions to chain after a groupby clause are: 

- `mean()`
- `sum()`
- `count()`
- `min()`
- `max()`

We can also use the `agg()` method to apply multiple aggregation functions at once. Let's by taking the count, min, and max of the `sepal_length` and `sepal_width` columns for each species.

```{python}
#| code-fold: false

df.groupby('species')[['sepal_length', 'sepal_width']].agg(['count', 'min', 'max'])
```

If you want to get summary statistics per group, you can also use the `describe()` method.

```{python}
#| code-fold: false

# let's get the summary statistics for all columns
# and show results for the petal_length column
df.groupby('species').describe()['petal_length']
```

## Handling missing values

In a perfect world, we would always work with complete data tables without any missing values. However, out in the wild we are often confronted with incomplete data, which is why dealing with missing values is an important skill to have. Pandas provides a few methods to handle missing values.

Let's look at our iris dataset and introduce some missing values.

```{python}
#| code-fold: false

# one missing value to the sepal_length column row 3
df.loc[2, 'sepal_length'] = np.nan
df.head()
```

There are a couple of methods which help us determine if missing values are present. The following methods are commonly used:

- `isnull()`: returns a DataFrame of the same shape as the original dataset, where each cell is either `True` or `False` depending on whether the value is missing or not.
- `notnull()`: returns the opposite of `isnull()`. It returns `True` if the value is not missing, and `False` otherwise.
- `isna()`: an alias for `isnull()`.
- `notna()`: an alias for `notnull()`.

The `isnull()` method returns a DataFrame of the same shape as the original dataset, where each cell is either `True` or `False` depending on whether the value is missing or not.

```{python}
#| code-fold: false

df.head().isnull()
```

The `isna()` method does exactly the same thing as `isnull()`, which we can see in the example below.
```{python}	
#| code-fold: false

df.head().isna()
```

The `notnull()` method returns the opposite of `isnull()`. It returns `True` if the value is not missing, and `False` otherwise.

```{python}
#| code-fold: false

df.head()['sepal_length'].notnull()
```

Related to missing values we can easily drop rows with missing values using the `dropna()` method. By default, it drops rows where at least one element is missing.

```{python}
#| code-fold: false

df.dropna().head()
```

The `dropna()` method also has a parameter `how` which can be set to `all`. This will only drop rows where all elements are missing. It is also possible to drop columns with missing values by setting the `axis` parameter to `1`. Let's see how this works.

```{python}
#| code-fold: false

# drop columns with missing values
df.dropna(axis=1).head()
```

### Filling missing values

Instead of dropping rows with missing values, we can also fill them with a specific value. This is ofter referred to as imputation. The `fillna()` method allows us to fill missing values with a specific value. Let's fill the missing values in the `sepal_length` column with the mean of the column.

```{python}
#| code-fold: false

# use the mean of sepal_length column to fill in for missing values
df['sepal_length'] = df['sepal_length'].fillna(df['sepal_length'].mean())
df.head()
```


### Missing values for other data types

So far, we have only looked at missing values in numerical columns. However, missing values can also occur in categorical columns. Let's introduce a missing value in the `species` column.

```{python}
#| code-fold: false

# introduce a missing value in the species column
df.loc[2, 'species'] = np.nan
df.head()
```

Let's inspect the datatypes of the columns in the dataset.

```{python}
#| code-fold: false

df.dtypes
```

We can now try to impute the most common value in the `species` column for the missing value.
Before we do that let's show to use the `mode()` method to get the most common value in a column.

```{python}	
#| code-fold: false

df['species'].mode()
```

We can see that mode returns a Series object. To get the actual value we can use the `iloc` method, or just use the `mode()[0]` method. Let's use this to fill the missing value in the `species` column.

```{python}
#| code-fold: false

# fill missing values in the species column with the most common value
df['species'] = df['species'].fillna(df['species'].mode()[0])
df.head()
```

Obviously, here we have imputed the incorrect species in place of the missing value. In practice, you would need to be more careful and consider the context of the data before imputing missing values. However, this example shows how you can impute missing values in categorical columns. There is more to learn about missing values, but for now, we will leave it at this.

## Joining DataFrames

Quite often we find ourselves working with multiple datasets that we need to combine. Pandas provides a few methods to join DataFrames. Let's start by creating two DataFrames to demonstrate how this works.

```{python}
#| code-fold: false

sales_regions = pd.DataFrame({
    'region': ['North', 'South', 'East', 'West'],
    'manager': ['John', 'Sara', 'Tom', 'Alice']
})

sales_regions
```

```{python}
#| code-fold: false

# sales for three years by region
sales_results = pd.DataFrame({
    'year': [2019, 2020, 2021, 2019, 2020, 2021, 
    2019, 2020, 2021, 2019, 2020, 2021],
    'region': ['North', 'North', 'North', 'South', 'South', 'South', 
    'East', 'East', 'East', 'West', 'West', 'West'],
    'sales': [1000, 1200, 1500, 800, 900, 1000, 
    700, 800, 900, 600, 700, 800]
})

sales_results
```

Given these DataFrames, might want to join them based on the `region` column. We could for example calculate the total sales per region, and add the information to the `sales_regions` DataFrame. We can do this using the `merge()` method. Let's see how this works.

```{python}
#| code-fold: false

# let's first calculate the total sales per region
sales_per_region = sales_results.groupby('region')['sales'].sum().reset_index()
sales_per_region
```

We used the `groupby()` method we learned earlier to calculate the total sales per region. We then used the `reset_index()` method to convert the grouped data back to a DataFrame. Now we can merge the `sales_regions` DataFrame with the `sales_per_region` DataFrame.

```{python}
#| code-fold: false

# merge the sales_regions DataFrame with the sales_per_region DataFrame
sales_regions.merge(sales_per_region, on='region', how='left')
```

If you have ever used SQL, you might have noticed that this feels very similar to a SQL join. The `on` parameter specifies the column to join on, and the `how` parameter specifies the type of join. Most common choices for the `how` parameter are probably `left`, `right`, `inner`, and `outer`. The default value is `inner`.

There is more functionality when it comes using `merge` for joining DataFrames, but for the time being we will leave it at this.

### Concatenating DataFrames

Now sometimes we have DataFrames where the content is different but the columns are the same. In this case we might want to concatenate the DataFrames. Let's assume we have sales data from previous years that we want to add to the `sales_results` DataFrame.

```{python}
#| code-fold: false

sales_results_2018 = pd.DataFrame({
    'year': [2018, 2018, 2018, 2018],
    'region': ['North', 'South', 'East', 'West'],
    'sales': [500, 400, 300, 200]
})

sales_results_2018
```

We can see that the format of this DataFrame is the same as the `sales_results` DataFrame. We can concatenate the two DataFrames using the `concat()` method, which takes a list of DataFrames as input.

```{python}
#| code-fold: false

# concatenate the sales results from different years
sales_all = pd.concat([sales_results_2018, sales_results])
sales_all
```

All looks good, but we can see that the index is not in order. We can reset the index using the `reset_index()` method.

```{python}
#| code-fold: false

sales_all.reset_index(drop=True, inplace=True)
sales_all
```

The `drop=True` parameter means that we want to get rid of the old index. The `inplace=True` parameter means that we want to modify the DataFrame in place, instead of creating a new DataFrame. This will make the index start from 0, and the changes are applied to the DataFrame.

### Concatenating along columns

Somethis we want to stick new columns at the end of an existing DataFrame. We can do this using the `concat()` method with the `axis` parameter set to 1. Let's look at an example.

```{python}
#| code-fold: false

sales_by_year = pd.DataFrame({
    'region': ['North', 'South', 'East', 'West'],
    '2018': [500, 400, 300, 200],	
    '2019': [1000, 800, 700, 600],
    '2020': [1200, 900, 800, 700]
})

sales_by_year
```

```{python}
#| code-fold: false
year_2021_2022 = pd.DataFrame({
    '2021': [1500, 1000, 900, 800],
    '2022': [1600, 1100, 1000, 900]
})

year_2021_2022
```

We can see that the `year_2021_2022` DataFrame is a natural extension to the `sales_by_year` DataFrame. Let's combine the two.

```{python}
#| code-fold: false

sales_by_year = pd.concat([sales_by_year, year_2021_2022], axis=1)
sales_by_year
```

### Joining DataFrames on the index values

So far we have joined DataFrames based on columns. However, we can also join DataFrames based on the index values. Let's see how this works by creating two DataFrames with the some overlapping index values.

```{python}
#| code-fold: false

df1 = pd.DataFrame({
    'A': [1, 2, 3],
    'B': [4, 5, 6]},
    index=[1, 2, 3])
df1
```

```{python}
#| code-fold: false

df2 = pd.DataFrame({
    'C': [7, 8, 9],
    'D': [10, 11, 12]},
    index=[2, 3, 4])

df2
```

We can now join the two DataFrames based on the index values using the `join()` method. Let's take a look at a left join.

```{python}
#| code-fold: false

df1.join(df2, how='left')
```

We can see that the `df1` DataFrame is the left DataFrame, which will be preserved in the join operation. The `df2` DataFrame is the right DataFrame, which will be joined to the left DataFrame based on the index values. If a value is missing in the right DataFrame, it will be filled with `NaN`.

A right join will preserve the right DataFrame.

```{python}
#| code-fold: false

df1.join(df2, how='right')
```
