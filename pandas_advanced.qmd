---
jupyter: python3
---

# Working with DataFrames

Now that we have learned how to create DataFrames, and some basic operations, let's deepen our understanding with a few more tricks that help us work with DataFrames more efficiently. We'll start of by importing the necessary libraries.

```{python}
#| code-fold: false
import numpy as np
import pandas as pd
```

Additionally, we will use a sample dataset to demonstrate the operations. Let's load the dataset from the seaborn library.

:::{.callout-note}
Seaborn is a commonly used library for data visualization in Python. It comes with a few sample datasets that we can use for practice. We'll use the classic `iris` dataset for this tutorial. We'll learn more about Seaborn in the upcoming chapters.
:::

```{python}
#| code-fold: false
import seaborn as sns

# Load the dataset
df = sns.load_dataset('iris')
df.head()
```

As we can see, the dataset contains four features: `sepal_length`, `sepal_width`, `petal_length`, and `petal_width`. The target variable is `species`, which is the type of iris flower. We also used the `head()` method to display the first five rows of the dataset. Let's learn more about the dataset using some common methods found in Pandas.

## Common methods in Pandas

We already saw that the `head()` method displays the first five rows of the dataset. Five is the default value, but we can change in using the `n` parameter. Similarly, we can use the `tail()` method to display the last rows of the dataset.

```{python}
#| code-fold: false

# Display the last nine rows
df.tail(9)
```

The `info()` method provides a concise summary of the dataset. It displays the number of non-null values in each column, the data type of each column, and the memory usage of the dataset.

```{python}
#| code-fold: false

df.info()
```

Summary statistics can be obtained using the `describe()` method. It provides the count, mean, standard deviation, minimum, maximum, and the quartiles of the dataset. This provides a handy way of getting an overall impression of the dataset.

```{python}
#| code-fold: false

df.describe()
```

If we just want to know the number of rows and columns in the dataset, we can use the `shape` attribute.

```{python}
#| code-fold: false

df.shape
```

The `columns` attribute provides the names of the columns in the dataset.

```{python}
#| code-fold: false

df.columns
```

The `nunique()` method provides the number of unique values in each column of the dataset.

```{python}
#| code-fold: false

df.nunique()
```

## Grouping data

If you are familiar with SQL queries, you might have used the `GROUP BY` clause to group data based on a particular column. Pandas provides a similar functionality using the `groupby()` method. Grouping data allows you to calculate for example statistics like the mean for distinct groups in the dataset. Let's take a look at how this works by grouping the `iris` data based on the `species` column and calculating the mean of the other columns.

```{python}
#| code-fold: false

df.groupby('species').mean()
```

We can also target specific columns. Let's see how this works by taking the sum of the `sepal_length` for each species.

```{python}
#| code-fold: false

df.groupby('species')['sepal_length'].sum()
```

Some commonly used aggregation functions to chain after a groupby clause are: 

- `mean()`
- `sum()`
- `count()`
- `min()`
- `max()`

We can also use the `agg()` method to apply multiple aggregation functions at once. Let's by taking the count, min, and max of the `sepal_length` and `sepal_width` columns for each species.

```{python}
#| code-fold: false

df.groupby('species')[['sepal_length', 'sepal_width']].agg(['count', 'min', 'max'])
```

If you want to get summary statistics per group, you can also use the `describe()` method.

```{python}
#| code-fold: false

# let's get the summary statistics for all columns
# and show results for the petal_length column
df.groupby('species').describe()['petal_length']
```

## Handling missing values

In a perfect world, we would always work with complete data tables without any missing values. However, out in the wild we are often confronted with incomplete data, which is why dealing with missing values is an important skill to have. Pandas provides a few methods to handle missing values.

Let's look at our iris dataset and introduce some missing values.

```{python}
#| code-fold: false

# one missing value to the sepal_length column row 3
df.loc[2, 'sepal_length'] = np.nan
df.head()
```

There are a couple of methods which help us determine if missing values are present. The following methods are commonly used:

- `isnull()`: returns a DataFrame of the same shape as the original dataset, where each cell is either `True` or `False` depending on whether the value is missing or not.
- `notnull()`: returns the opposite of `isnull()`. It returns `True` if the value is not missing, and `False` otherwise.
- `isna()`: an alias for `isnull()`.
- `notna()`: an alias for `notnull()`.

The `isnull()` method returns a DataFrame of the same shape as the original dataset, where each cell is either `True` or `False` depending on whether the value is missing or not.

```{python}
#| code-fold: false

df.head().isnull()
```

The `isna()` method does exactly the same thing as `isnull()`, which we can see in the example below.
```{python}	
#| code-fold: false

df.head().isna()
```

The `notnull()` method returns the opposite of `isnull()`. It returns `True` if the value is not missing, and `False` otherwise.

```{python}
#| code-fold: false

df.head()['sepal_length'].notnull()
```

Related to missing values we can easily drop rows with missing values using the `dropna()` method. By default, it drops rows where at least one element is missing.

```{python}
#| code-fold: false

df.dropna().head()
```

The `dropna()` method also has a parameter `how` which can be set to `all`. This will only drop rows where all elements are missing. It is also possible to drop columns with missing values by setting the `axis` parameter to `1`. Let's see how this works.

```{python}
#| code-fold: false

# drop columns with missing values
df.dropna(axis=1).head()
```

### Filling missing values

Instead of dropping rows with missing values, we can also fill them with a specific value. This is ofter referred to as imputation. The `fillna()` method allows us to fill missing values with a specific value. Let's fill the missing values in the `sepal_length` column with the mean of the column.

```{python}
#| code-fold: false

# use the mean of sepal_length column to fill in for missing values
df['sepal_length'] = df['sepal_length'].fillna(df['sepal_length'].mean())
df.head()
```


### Missing values for other data types

So far, we have only looked at missing values in numerical columns. However, missing values can also occur in categorical columns. Let's introduce a missing value in the `species` column.

```{python}
#| code-fold: false

# introduce a missing value in the species column
df.loc[2, 'species'] = np.nan
df.head()
```

Let's inspect the datatypes of the columns in the dataset.

```{python}
#| code-fold: false

df.dtypes
```

We can now try to impute the most common value in the `species` column for the missing value.
Before we do that let's show to use the `mode()` method to get the most common value in a column.

```{python}	
#| code-fold: false

df['species'].mode()
```

We can see that mode returns a Series object. To get the actual value we can use the `iloc` method, or just use the `mode()[0]` method. Let's use this to fill the missing value in the `species` column.

```{python}
#| code-fold: false

# fill missing values in the species column with the most common value
df['species'] = df['species'].fillna(df['species'].mode()[0])
df.head()
```

