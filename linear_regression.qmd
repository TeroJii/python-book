---
jupyter: python3
---

# Linear regression

Linear regression is fancy way of describing the act of fitting a line through your data. What makes it powerful is it's simplicity. When we are facing a regression problem this is usually the first thing we should try before moving on to more complex models. If you are looking for a detailed explanation on the intricacies of linear regression, I recommend taking a closer look at the wonderful book by James, Witten, Hastie and Tibshirani titled `An Introduction to Statistical Learning` (@ISLP).

::: {.callout-tip}
## Regression in a nutshell

In regression we are trying to predict a numerical (continuous) variable based on one or more other variables. The variable we are trying to predict is called the dependent variable, and the variables we are using to make the prediction are called the independent variables. In the field of Data Science, the dependent variable can sometimes be called the outcome, and the independent variables might be referred to as features.
:::

## Simple linear regression

Simple linear regression is the most basic form of regression. It is used when we want to use the values of a single independent variable to predict the values for the dependent variable. To put it simply, the goal is to find the best-fitting straight line through the data. The equation for a simple linear regression model is:

$$
y = \beta_0 + \beta_1x \text{,}
$$ {#eq-linear}

where $y$ is the dependent variable, $x$ is the independent variable, $\beta_0$ is the intercept, $\beta_1$ is the slope.

Let's see how we can implement simple linear regression in Python using the `scikit-learn` library. Scikit-learn is one of the most fundamental machine learning libraries for Python. It provides a wide range of tools for building machine learning models, including regression models. Before we start, we need to install the library. You can do this by running the following command in your terminal:

```bash
pip install scikit-learn
```

After installation, we are ready to implement a simple linear regression model. 
We will use data from the World Happiness Report 2019, which is available through Kaggle (@HappinessData2019). 
The World Happiness Report is a survey of the state of global happiness that ranks 156 countries by how happy their citizens perceive themselves to be. 
The dataset contains information about various factors that contribute to happiness, such as GDP per capita, social support, healthy life expectancy, freedom to make life choices, generosity, and perceptions of corruption (@tbl-happiness).

```{python}
#| label: tbl-happiness
#| tbl-cap: The first rows of the World Happiness Report 2019 dataset.
#| code-fold: false

import pandas as pd
import seaborn as sns

# get the diamonds dataset
happiness = pd.read_csv('data/WorldHappinessReport2019.csv')
happiness.head()
```

@fig-lm_happy visualizes the relationship between a country's happiness `Score` and `GDP per capita`. We can see that, on average countries with higher GDP per capita exhibit higher levels of happiness.

```{python}
#| label: fig-lm_happy
#| code-fold: false
#| fig-cap: A scatter plot showing the relationship between the happiness score and the GDP per capita. Note that correlation does not imply causation.

sns.lmplot(x='GDP per capita', y='Score', data=happiness)

```

### Fitting the model with scikit-learn

Above we used our `seaborn` knowledge to visualize the linear relationship. However, this doesn't actually give as a model with stored parameter values. In order to actually fit the model shown in @eq-linear, we can use `LinearRegression` from `scikit-learn`.

```{python}
#| code-fold: false

from sklearn.linear_model import LinearRegression

# create a linear regression model object
model = LinearRegression()

# fit the model to the data
model.fit(X=happiness[['GDP per capita']], y=happiness['Score'])

# print the model coefficients
model.coef_, model.intercept_
```

Let's go throught the code above line by line. First, we import the `LinearRegression` Estimator from the `sklearn.linear_model` family of models. Next, we create a `LinearRegression` object called `model`. We then fit the model to the data using the `fit` method. The `fit` method takes two arguments: the independent variable `X` and the dependent variable `y`. In this case, we are using `GDP per capita` as the independent variable and `Score` as the dependent variable. Finally, we print the model coefficients. The `coef_` attribute contains the slope of the line, and the `intercept_` attribute contains the intercept.

Let's now look at @fig-lm_happy again, to help us understand the model we just fitted. 
The slope of the line is the coefficient for `GDP per capita` ($\beta_1$ in @eq-linear), and the intercept is the value of the dependent variable when `GDP per capita` is zero.
In other words, the intercept is the y-value where the fitted line crosses the y-axis. By looking at the plot, we can see that the intercept is around 3.4, which is the result we got from our fitted model as well.
The slope is around 2.2, which means that for every unit increase in `GDP per capita`, the `Score` increases by 2.2 units. This is the beauty of linear regression - it gives us a simple and interpretable model that we can use to make predictions.

### Making predictions

