---
jupyter: python3
---

# Logistic regression

In the previous chapter we learned about linear regression. The word regression is used in the context of statistical modelling to refer to the act of estimating the relationship between a dependent variable and one or more independent variables. However, sometimes what we are trying to predict might not be a continuous numerical variable, but categorical. In such cases, instead of a regression problem, we are dealing with classification. In the simplest case, there are only two distinct classes. This is called binary classification. One of the simplests algorithms for solving these types of problems is called logistic regression, which despite its name, is used for classification.

## Univariate logistic regression

In univariate logistic regression, we have a single independent variable and a binary dependent variable. The goal is to estimate the probability that the dependent variable is 1, based on the value of the independent variable. This implies that the values of the categorical dependent variable are assigned so-called dummy values, and are therefore coded as 0 and 1. For example, if we are trying to predict whether a student will pass or fail an exam based on the number of hours they studied, we can code the dependent variable as 1 if the student passed and 0 if they failed.

The logistic function is used to model the relationship between the inpedendent and dependent variables. The logistic function is defined as:

$$
y = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x)}},
$$ {#eq-logistic}

where $y$ is again the dependent variable, and $x$ is the independent variable, and $\beta_0$ and $\beta_1$ are the parameters of the model. As you might have noticed, the exponential term contains the linear regression equation. This is not a coincidence. The logistic regression uses something called the sigmoid function, which basically transforms the output of the linear regression into a probability. The sigmoid function is defined as:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}.
$$ {#eq-sigmoid}

If we plot out the sigmoid function, we can see that it has a distinct shape, which somewhat resembles the letter S. @fig-sigmoid shows the sigmoid function with $z$ on the x-axis and $\sigma(z)$ on the y-axis.

```{python}
#| label: fig-sigmoid
#| fig-cap: The shape of the sigmoid function has a distinct S-like shape.

import numpy as np
import matplotlib.pyplot as plt

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

z = np.linspace(-10, 10, 100)

plt.plot(z, sigmoid(z))
plt.xlabel('z')
plt.ylabel('Ïƒ(z)')
plt.title('Sigmoid function')

plt.show()
```

The beauty of the Sigmoid function is that it coerces any input to the variable $z$ to lie between 0 and 1. This is exactly what we need for a probability. In other words, if the values we are trying to predict are binary, with values 0 and 1, the sigmoid function will essentially give us the probability that the value is 1.

## Fitting a univariate logistic regression model

Let's see how we can fit a logistic regression model in practice. For this, we will explore the  [UCI ML Repository Breast cancer dataset](https://archive.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original), which contains information about breast cancer patients (@UCIBreastCancer1992). **Add one more citation...**
@tbl-bc_data shows the first few rows of the dataset. The `Class` column contains the dependents variable. Benign cases are labelled with 2, whereas malignant cases are labelled with 4.

```{python}
#| label: tbl-bc_data
#| tbl-cap: The first few rows of the slightly modified UCI ML Repository original Breast cancer dataset.
import pandas as pd

bc_data = pd.read_csv('data/uci_bc_mod.csv')

bc_data.head()
```

### Visualizing the data

Let's start by plotting the `Uniformity_of_cell_size` variable against the `Class` variable.
We'll recode the class variable so that benign cases are labelled as 0 and malignant cases as 1.
@fig-bc_data shows the relationship between the two variables with a logistic model fit using Seaborn.  

```{python}
#| label: fig-bc_data
#| fig-cap: The relationship between the uniformity of cell size and the class variable in the Breast cancer dataset. The line represents a univariate logistic regression fit, which predicts that the larger the recorded value for the uniformity of cell size, the higher the probability for the class being malignant.

import seaborn as sns

# let's recode Class labels as 0 (=2) and 1 (=4)
bc_data['Class'] = bc_data['Class'].map({2: 0, 4: 1})

sns.lmplot(data=bc_data, x='Uniformity_of_cell_size', 
y='Class', logistic=True, ci=95, height=4, aspect=1.5)
```	

Just by looking at the scatter plot, it is not obvious to tell what the relationship between the two variables is. 
However, the logistic regression line helps us to see the relationship more clearly. Let's plot the class distributions as histograms to clarify the relationship between the two variables (@fig-bc_hist).

```{python}
#| label: fig-bc_hist
#| fig-cap: The histograms show the distribution of the uniformity of cell size for benign and malignant cases. The distribution of the uniformity of cell size is skewed towards higher values for malignant cases.

sns.histplot(data=bc_data, x='Uniformity_of_cell_size', hue='Class', 
bins=20, kde=True)
```

The histogram clarifies why the logistic regression line is shaped the way it is. Most benign cases appear to have smaller values for the uniformity of cell size.

### Fitting the model

Above we visualized the logistic relationship between the `Uniformity_of_cell_size` variable and the `Class` variable using `seaborn`.
We are now ready to use `scikit-learn` to fit a logistic regression model to the data. 
Let's start by splitting the data into training and testing sets.


```{python}
#| code-fold: false

from sklearn.model_selection import train_test_split

X = bc_data[['Uniformity_of_cell_size']]
y = bc_data['Class']

# train test split
X_train, X_test, y_train, y_test = train_test_split(X, 
y, test_size=0.3, random_state=42, stratify=y)
```

We used the `train_test_split` function again to split the data into training and testing sets. 
The `stratify` parameter ensures that the distribution of the target variable is the same in both train and test sets. 
This is important if the target variable displays a class imbalance. 
In our case the target variable is not terribly unbalanced, but it is still good practice to learn to use the `stratify` parameter.

```{python}
#| code-fold: false

# class distribution for benign and malignant cases
bc_data['Class'].value_counts()
```

We can now use the `LogisticRegression` class from the `linear_model` module. 
The `fit` method is used to train the model. Summary of the model parameters is shown in @tbl-model_summary.

```{python}
#| code-fold: false

# fitting the model
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
```

```{python}
#| label: tbl-model_summary
#| tbl-cap: Summary of the logistic regression model parameters.

model_summary = pd.DataFrame(
    data=np.array([log_reg.intercept_, log_reg.coef_[0]]),
    index=['Intercept', 'Uniformity_of_cell_size'], 
    columns=['Coefficient'])

model_summary
```

### Making predictions

We have managed to fit a model to the training data, but thus far our only result is the model parameters, which are quite abstract.
To get a better sense of how well we are doing, in terms of prediction accuracy, let's make predictions on our test data.
As you might recall from the previous chapter, we can use the `predict` method to make predictions.

```{python}
#| code-fold: false

# making predictions
y_pred = log_reg.predict(X_test)

y_pred[:5]
```


### Evaluating the model

Something about the confusion matrix.