---
jupyter: python3
---

# Decision Trees

Decision trees are a popular machine learning algorithm that can be used for both classification and regression tasks. They are easy to understand and interpret, and they can handle both numerical and categorical data.
Moreover, some more advanced algorithms are based on decision trees, such as Random Forests and Gradient Boosting.

Let's illustrate the concept with a classification example.

## Decision Trees for Classification

We will use the (in)famous Iris dataset, which contains information about three different species of iris flowers. The goal is to classify the species based on the sepal length, sepal width, petal length, and petal width of the flowers.
@tbl-iris-data shows a preview with a few first rows from the dataset.

```{python}
#| label: tbl-iris-data
#| tbl-cap: First few rows of the Iris dataset
 
# Load the Iris dataset
from sklearn.datasets import load_iris
iris = load_iris()

# Create a DataFrame with the feature variables
import pandas as pd
data = pd.DataFrame(iris.data, columns=iris.feature_names)

# Add the target variable to the DataFrame
data['species'] = iris.target

# Display the first few rows of the DataFrame
data.head()
```

Now let's say we would want to predict the species of an iris flower based on its sepal length and sepal width. We can train a decision tree classifier on the data and visualize the decision boundaries.

### Building a Decision Tree Classifier

We can fit a decision tree to our data with the help of `scikit-learn`. The first step, as always, is to select the feature variables and split the data into training and testing sets.

```{python}
#| code-fold: false
#| label: train-decision-tree

# Load the necessary libraries
from sklearn import tree
from sklearn.model_selection import train_test_split

# Select the feature variables
X = data[['sepal length (cm)', 'sepal width (cm)']]
y = data['species']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, 
test_size=0.25, random_state=42)

# Create a decision tree classifier
tree_classifier = tree.DecisionTreeClassifier(max_depth=3)

# Train the classifier on the training data
tree_classifier.fit(X_train, y_train)
```

Above we used the `tree` module from `scikit-learn` to create a decision tree classifier with a maximum depth of 3. We will see in a moment what this means. The classifier is then trained on the training data. We left 25% of the data as our test set. The `predict` method allows us to, well, predict the species of the flowers in the test set:

```{python}
#| label: make-predictions
#| code-fold: false

# Make predictions on the test data
y_pred = tree_classifier.predict(X_test)

print(y_pred)
```

The test predictions above are stored in a variable `y_pred`. We can now evaluate the model's performance by computing the confusion matrix. @fig-confusion-matrix shows the confusion matrix for the decision tree classifier.

```{python}	
#| label: fig-confusion-matrix
#| fig-cap: Confusion matrix showing the performance of the decision tree classifier on the test data.

# Load the necessary libraries
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, 
fmt='d', cmap='Blues', cbar=False, 
xticklabels=iris.target_names, yticklabels=iris.target_names)
plt.xlabel('Predicted species')
plt.ylabel('Actual species')
plt.title('Confusion Matrix')
plt.show()
```

Let's visualize the decision boundaries.

### Visualizing the Decision Boundaries

The decision boundaries of a decision tree classifier can be visualized using the `plot_tree` function from the `sklearn.tree` module. The function takes the trained classifier, feature names, class names, and other optional parameters as input. @fig-decision-boundaries shows the decision boundaries of the trained decision tree classifier.

```{python}
#| label: fig-decision-boundaries
#| fig-cap: The visualization of the decision tree shows how the classification predictions are made based on the input features. Each node represents a decision based on a feature, and the color represents the predicted class.

tree.plot_tree(tree_classifier, feature_names=['sepal length (cm)', 'sepal width (cm)'], class_names=iris.target_names, filled=True,
rounded=True, fontsize=7,
label='root',
impurity=False)
plt.show()
```

Visualizing the decision boundaries can help us understand how the decision tree classifier makes predictions based on the input features.
