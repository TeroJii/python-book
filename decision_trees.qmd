---
jupyter: python3
---

# Decision Trees

Decision trees are a popular machine learning algorithm that can be used for both classification and regression tasks. They are easy to understand and interpret, and they can handle both numerical and categorical data.
Moreover, some more advanced algorithms are based on decision trees, such as Random Forests and Gradient Boosting.

Let's illustrate the concept with a classification example.

## Decision Trees for Classification

We will use the (in)famous Iris dataset, which contains information about three different species of iris flowers. The goal is to classify the species based on the sepal length, sepal width, petal length, and petal width of the flowers.
@tbl-iris-data shows a preview with a few first rows from the dataset.

```{python}
#| label: tbl-iris-data
#| tbl-cap: First few rows of the Iris dataset
 
# Load the Iris dataset
from sklearn.datasets import load_iris
iris = load_iris()

# Create a DataFrame with the feature variables
import pandas as pd
data = pd.DataFrame(iris.data, columns=iris.feature_names)

# Add the target variable to the DataFrame
data['species'] = iris.target

# Display the first few rows of the DataFrame
data.head()
```

Now let's say we would want to predict the species of an iris flower based on its sepal length and sepal width. We can train a decision tree classifier on the data and visualize the decision boundaries.

### Building a Decision Tree Classifier

We can fit a decision tree to our data with the help of `scikit-learn`. The first step, as always, is to select the feature variables and split the data into training and testing sets.

```{python}
#| code-fold: false
#| label: train-decision-tree

# Load the necessary libraries
from sklearn import tree
from sklearn.model_selection import train_test_split

# Select the feature variables
X = data[['sepal length (cm)', 'sepal width (cm)']]
y = data['species']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, 
test_size=0.25, random_state=42)

# Create a decision tree classifier
tree_classifier = tree.DecisionTreeClassifier(max_depth=3)

# Train the classifier on the training data
tree_classifier.fit(X_train, y_train)
```

Above we used the `tree` module from `scikit-learn` to create a decision tree classifier with a maximum depth of 3. We will see in a moment what this means. The classifier is then trained on the training data. We left 25% of the data as our test set. The `predict` method allows us to, well, predict the species of the flowers in the test set:

```{python}
#| label: make-predictions
#| code-fold: false

# Make predictions on the test data
y_pred = tree_classifier.predict(X_test)

print(y_pred)
```

The test predictions above are stored in a variable `y_pred`. We can now evaluate the model's performance by computing the confusion matrix. @fig-confusion-matrix shows the confusion matrix for the decision tree classifier.

```{python}	
#| label: fig-confusion-matrix
#| fig-cap: Confusion matrix showing the performance of the decision tree classifier on the test data.

# Load the necessary libraries
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, 
fmt='d', cmap='Blues', cbar=False, 
xticklabels=iris.target_names, yticklabels=iris.target_names)
plt.xlabel('Predicted species')
plt.ylabel('Actual species')
plt.title('Confusion Matrix')
plt.show()
```

We can tell by looking at the confusion matrix that the model seems quite capable of distinguishing between the three species of iris flowers. But how does it actually achieve this? Let's visualize the decision making process.

### Visualizing the Decision Boundaries

Decision trees are called so due to the visual appearance of the model. The tree consist of nodes that represent decisions based on the input features.
We can visualize the nodes an splits in the data with the `plot_tree` function from the `sklearn.tree` module. @fig-decision-boundaries shows the decision boundaries of the trained decision tree classifier.

```{python}
#| label: fig-decision-boundaries
#| fig-cap: The visualization of the decision tree shows how the classification predictions are made based on the input features. Each node represents a decision based on a feature, and the color represents the predicted class.
#| code-fold: false

tree.plot_tree(
    tree_classifier, 
    feature_names=['sepal length (cm)', 'sepal width (cm)'], 
    class_names=iris.target_names, 
    filled=True,
    rounded=True, fontsize=7,
    label='root', impurity=False
    )
plt.show()
```

Visualizing the decision boundaries can help us understand how the decision tree classifier makes predictions based on the input features. For example, from the @fig-decision-boundaries we can see that the classifier first checks if the sepal length is less than 5.45 cm. If it is, the flower is classified as `setosa`. Observations where sepal length is equal to or above 5.45 cm are classified as `virginica`. Similar decisions are made twice more, which corresponds to the `max_depth=3` parameter we set when creating the classifier. This decision making process categorizes our data into three distinct classes. But how is the algorithm able to make these decisions?

## Decision Trees for Regression

Decision trees can be applied to regression problems as well. Our workflow will be almost analogous to the one we used for classification. We will use the California housing dataset, which contains information about various features of houses in California and their corresponding prices. The goal is to predict the price of a house based on its features. @tbl-california-housing shows a preview with a few first rows from the dataset.

```{python}
#| label: tbl-california-housing
#| tbl-cap: First few rows of the California housing dataset

# Load the California housing dataset
from sklearn.datasets import fetch_california_housing
california_housing = fetch_california_housing()

data = pd.DataFrame(california_housing.data, columns=california_housing.feature_names)
data['MedVal'] = california_housing.target

data.head()
```

```{python}
#| label: train-decision-tree-regression


X = california_housing.data
y = california_housing.target
X_train, X_test, y_train, y_test = train_test_split(X, y,
test_size=0.25, random_state=42)
# Create a decision tree regressor
from sklearn.tree import DecisionTreeRegressor
tree_regressor = DecisionTreeRegressor(max_depth=3)
# Train the regressor on the training data
tree_regressor.fit(X_train, y_train)
# Make predictions on the test data
y_pred = tree_regressor.predict(X_test)
# Compute the mean squared error
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse:.2f}')
```

## How Decision Trees Work

WIP...

The Gini index is a measure of impurity used by the decision tree algorithm to determine the best split at each node. The Gini index is calculated as follows:

$$
G = 1 - \sum_{i=1}^{n} p_i^2
$$

where $p_i$ is the probability of observing class $i$ in a given node. The Gini index ranges from 0 to 1, where 0 indicates that the node is pure (i.e., all observations belong to the same class) and 1 indicates that the node is impure (i.e., observations are evenly distributed among classes).

Entropy is another measure of impurity that can be used by the decision tree algorithm. The entropy of a node is calculated as follows:

$$
H = -\sum_{i=1}^{n} p_i \log_2(p_i)
$$

where $p_i$ is the probability of observing class $i$ in a given node. The entropy ranges from 0 to $\log_2(n)$, where 0 indicates that the node is pure (i.e., all observations belong to the same class) and $\log_2(n)$ indicates that the node is impure (i.e., observations are evenly distributed among classes).

WIP...